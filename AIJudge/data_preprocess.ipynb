{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600589758236",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "明略数据比赛：https://www.datafountain.cn/competitions/277/datasets  \n",
    "数据格式  \n",
    "比赛中提供的训练数据为多行文本，每一行分为四列，使用\\t分割，第一列为文档ID, 第二列为案件事实描述，第三列为罚金额度类别，第四列为对应的法律条文编号序列，其中法律条文编号序列是有”,”号分割。\n",
    "数据样例如下，以表格形式进行表示：  \n",
    "1  \n",
    "被告人高某明知罂粟为毒品，于2012年11月份将捡拾的罂粟籽种植在其院内。2013年4月8日，公安机关民警在巡逻中发现被告人高某家中种植的罂粟后遂将罂粟植株予以铲除、扣押。经现场清点，高某种植的罂粟植株共计622株。  \n",
    "1  \n",
    "351,67,72,73  \n",
    "另外会提供训练数据中相关的法律条文内容，所有条文按编号排列。每条法律条文内容占一行。  \n",
    "注：罚金金额与其类别标签的对应关系如下：  \n",
    "金额范围(元) 类别标签  \n",
    "(,1000] 1  \n",
    "(1000,2000] 2  \n",
    "(2000, 3000] 3  \n",
    "(3000,4000] 4  \n",
    "(4000, 5000] 5  \n",
    "(5000, 10000] 6  \n",
    "(10000,50w] 7  \n",
    "(50w, ) 8  \n",
    "\n",
    "数据规模  \n",
    "初赛数据规模：训练数据规模4w，A榜测试数据规模1w，B榜测试数据2w  \n",
    "复赛数据规模：训练数据规模12w，A榜测试数据规模3w, B榜测试数据6w  \n",
    "\n",
    "评分方式  \n",
    "参赛者提交的结果文件中包含对每个文档罚金类别预测结果和对应法律条文标号。对于两类结果，本赛题进行单独评测。针对罚金类别的预测结果，使用 Micro-Averaged F1指标衡量模型性能。其定义如下：  \n",
    "$micro-avg-F1=\\frac{1}{N}\\sum_{i=1}^{m}w_if_i$  \n",
    "其中， m表示类别数目，N表示样例总数， 表示测试样例中属于第i类的个数， f1i是第i类中的F1值。  \n",
    "针对法律条款预测结果，使用Jaccard相似系数衡量，其定义如下：  \n",
    "$ P_i = \\frac{|L^i \\bigcap L_g^i|}{|L^i \\bigcup L_g^i|}$  \n",
    "$ p = \\frac{1}{N}\\sum_{i=1}^np_i$\n",
    "\n",
    "其中$L^i$，：第i题的标准答案的法律条文集合；$L_g^i$：第i题的用户输出的法律条文集合  \n",
    "\n",
    "具体排名规则如下：  \n",
    "初赛排名仅以micro_avg_F1值进行排名；  \n",
    "复赛排名规则则综合两项结果的性能指标，具体计方式如下：0.5 * micro_avg_F1 + 0.5 p。  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = True\n",
    "if test:\n",
    "    nrows = 100\n",
    "else:\n",
    "    nrows = None\n",
    "\n",
    "df_first = pd.read_csv(os.path.join(path, './data/first_round/1-train/train.txt'), sep='\\t', names=['ID', 'text', 'penalty', 'items'], nrows=nrows)\n",
    "df_second = pd.read_csv(os.path.join(path, './data/second_round/train.txt'), sep='\\t', names=['ID', 'text', 'penalty', 'items'], nrows=nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_first.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_second.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_second.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_first, df_second])"
   ]
  }
 ]
}