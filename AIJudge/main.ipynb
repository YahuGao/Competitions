{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import preprocess\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train_splited.csv'\n",
    "test_path = './data/test_splited.csv'\n",
    "import os\n",
    "nrows = None\n",
    "if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
    "    # 加载训练集和测试集\n",
    "    df_tr, df_te = preprocess.load_data(nrows=None)\n",
    "    \n",
    "    df_tr['penalty'] = df_tr['penalty'].parallel_apply(lambda x:x-1)\n",
    "    df_te['penalty'] = df_te['penalty'].parallel_apply(lambda x:x-1)\n",
    "    \n",
    "    df_tr.to_csv('./data/train_splited.csv', sep='\\t', index=False)\n",
    "    df_te.to_csv('./data/test_splited.csv', sep='\\t', index=False)\n",
    "else:\n",
    "    df_tr = pd.read_csv(train_path, sep='\\t', nrows=nrows)\n",
    "    df_te = pd.read_csv(test_path, sep='\\t', nrows=nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看列信息\n",
    "df_tr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练集和测试集结合起来\n",
    "df_all = pd.concat([df_tr, df_te])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义罚金类别的评估函数\n",
    "from sklearn.metrics import f1_score\n",
    "def micro_avg_f1(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从训练集和测试集中的文本训练\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 当设置为浮点数时，过滤出现在超过max_df/低于min_df比例的句子中的词语；正整数时,则是超过max_df句句子\n",
    "tfv = TfidfVectorizer(#analyzer='word',\n",
    "                      # analyzer是word时默认只匹配长度大于2的词，且自动屏蔽标点， 这回导致文本中的罚金1,000和小数 1.5被分开\n",
    "                      # \\w+ 这里能够取出单个的字 但仍然匹配不到小数 待优化 先跑通baseline\n",
    "                      token_pattern=r'(?u)\\b\\w+\\b',             \n",
    "                      ngram_range=(1,3),\n",
    "                      min_df=3, max_df=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.95, min_df=3, ngram_range=(1, 3),\n",
       "                token_pattern='(?u)\\\\b\\\\w+\\\\b')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用全部数据进行tfidf的转换 不合适 应当使用训练集的数据提取转换器\n",
    "# tfv.fit(df_all.text.values.tolist())\n",
    "tfv.fit(df_tr.text.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = tfv.transform(df_tr.text.values.tolist())      # 将输入文本转换为tf-idf表示形式\n",
    "train_y = df_tr.penalty.values\n",
    "# penalty_classes = len(df_tr.penalty.unique())            # 罚金类别种类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def log(stri):\n",
    "    now = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "    print(str(now) + ' ' + str(stri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def model_cv(model, skf, train_x, train_y):\n",
    "    penalty_classes = len(set(train_y))\n",
    "    stack = np.zeros((train_x.shape[0], penalty_classes))\n",
    "    score_va = 0\n",
    "\n",
    "    for i, (tr, va) in enumerate(skf.split(train_x, train_y)):\n",
    "        log('stack: %d/%d'%((i+1), n_folds))\n",
    "        model.fit(train_x[tr], train_y[tr])\n",
    "        predict_va = model.predict_proba(train_x[va])         # 划分出的验证集预测各类别概率\n",
    "        log('va acc:%f' % micro_avg_f1(train_y[va], model.predict(train_x[va])))         # 求出验证集的预测精度\n",
    "        score_va += micro_avg_f1(train_y[va], model.predict(train_x[va]))                # 验证集的f1-score， micro\n",
    "        stack[va] += predict_va\n",
    "\n",
    "    score_va /= n_folds\n",
    "    log('va avg acc:%f' % score_va)\n",
    "\n",
    "    return stack\n",
    "\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=2, n_jobs=-1, solver='sag', multi_class='ovr')\n",
    "stack_lr = model_cv(lr, skf, train_x, train_y)\n",
    "\n",
    "df_stack = pd.DataFrame(index=range(len(stack_lr)))\n",
    "for i in range(stack_lr.shape[1]):\n",
    "    df_stack['tfidf_lr_{}'.format(i)] = stack_lr[:, i]\n",
    "\n",
    "df_stack.to_csv('./data/lr_prob.csv', index=None, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "stack_bnb = model_cv(bnb, skf, train_x, train_y)\n",
    "df_stack = pd.DataFrame(index=range(len(stack_bnb)))\n",
    "for i in range(stack_bnb.shape[1]):\n",
    "    df_stack['tfidf_svc_{}'.format(i)] = stack_bnb[:, i]\n",
    "\n",
    "df_stack.to_csv('./data/bnb_prob.csv', index=None, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "stack_mnb = model_cv(mnb, skf, train_x, train_y)\n",
    "df_stack = pd.DataFrame(index=range(len(stack_mnb)))\n",
    "for i in range(stack_mnb.shape[1]):\n",
    "    df_stack['tfidf_mnb_{}'.format(i)] = stack_mnb[:, i]\n",
    "\n",
    "df_stack.to_csv('./data/mnb_prob.csv', index=None, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-21 16:52:30 stack: 1/5\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yahu/anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-21 17:44:01 va acc:0.631680\n",
      "2020-09-21 17:44:01 stack: 2/5\n",
      "[LibLinear]2020-09-21 18:41:02 va acc:0.517656\n",
      "2020-09-21 18:41:03 stack: 3/5\n",
      "[LibLinear]2020-09-21 19:41:32 va acc:0.523516\n",
      "2020-09-21 19:41:32 stack: 4/5\n",
      "[LibLinear]2020-09-21 20:38:38 va acc:0.509805\n",
      "2020-09-21 20:38:39 stack: 5/5\n",
      "[LibLinear]2020-09-21 21:31:49 va acc:0.511914\n",
      "2020-09-21 21:31:49 va avg acc:0.538914\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svc = svm.LinearSVC(loss='hinge', tol=0.000001, C=0.5, verbose=1, random_state=2020, max_iter=5000)\n",
    "classes = len(set(train_y))\n",
    "stack = np.zeros((train_x.shape[0], classes))\n",
    "score_va = 0\n",
    "\n",
    "for i, (tr, va) in enumerate(skf.split(train_x, train_y)):\n",
    "    log('stack: %d/%d'%((i+1), n_folds))\n",
    "    svc.fit(train_x[tr], train_y[tr])\n",
    "    predict_va = svc.decision_function(train_x[va])         # 划分出的验证集预测各类别概率\n",
    "    log('va acc:%f' % micro_avg_f1(train_y[va], svc.predict(train_x[va])))         # 求出验证集的预测精度\n",
    "    score_va += micro_avg_f1(train_y[va], svc.predict(train_x[va]))                # 验证集的f1-score， micro\n",
    "    stack[va] += predict_va\n",
    "\n",
    "score_va /= n_folds\n",
    "log('va avg acc:%f' % score_va)\n",
    "stack_svc = stack\n",
    "df_stack = pd.DataFrame(index=range(len(stack_svc)))\n",
    "for i in range(stack_svc.shape[1]):\n",
    "    df_stack['tfidf_svc_{}'.format(i)] = stack_svc[:, i]\n",
    "\n",
    "df_stack.to_csv('./data/svc_prob.csv', index=None, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-21 22:35:37 iter = 1\n",
      "2020-09-21 22:35:37 iter = 1001\n",
      "2020-09-21 22:35:37 iter = 2001\n",
      "2020-09-21 22:35:38 iter = 3001\n",
      "2020-09-21 22:35:38 iter = 4001\n",
      "2020-09-21 22:35:38 iter = 5001\n",
      "2020-09-21 22:35:39 iter = 6001\n",
      "2020-09-21 22:35:39 iter = 7001\n",
      "2020-09-21 22:35:39 iter = 8001\n",
      "2020-09-21 22:35:39 iter = 9001\n",
      "2020-09-21 22:35:40 iter = 10001\n",
      "2020-09-21 22:35:40 iter = 11001\n",
      "2020-09-21 22:35:40 iter = 12001\n",
      "2020-09-21 22:35:41 iter = 13001\n",
      "2020-09-21 22:35:41 iter = 14001\n",
      "2020-09-21 22:35:41 iter = 15001\n",
      "2020-09-21 22:35:42 iter = 16001\n",
      "2020-09-21 22:35:42 iter = 17001\n",
      "2020-09-21 22:35:42 iter = 18001\n",
      "2020-09-21 22:35:43 iter = 19001\n",
      "2020-09-21 22:35:43 iter = 20001\n",
      "2020-09-21 22:35:43 iter = 21001\n",
      "2020-09-21 22:35:44 iter = 22001\n",
      "2020-09-21 22:35:44 iter = 23001\n",
      "2020-09-21 22:35:44 iter = 24001\n",
      "2020-09-21 22:35:45 iter = 25001\n",
      "2020-09-21 22:35:45 iter = 26001\n",
      "2020-09-21 22:35:45 iter = 27001\n",
      "2020-09-21 22:35:45 iter = 28001\n",
      "2020-09-21 22:35:46 iter = 29001\n",
      "2020-09-21 22:35:46 iter = 30001\n",
      "2020-09-21 22:35:46 iter = 31001\n",
      "2020-09-21 22:35:47 iter = 32001\n",
      "2020-09-21 22:35:47 iter = 33001\n",
      "2020-09-21 22:35:47 iter = 34001\n",
      "2020-09-21 22:35:48 iter = 35001\n",
      "2020-09-21 22:35:48 iter = 36001\n",
      "2020-09-21 22:35:48 iter = 37001\n",
      "2020-09-21 22:35:49 iter = 38001\n",
      "2020-09-21 22:35:49 iter = 39001\n",
      "2020-09-21 22:35:49 iter = 40001\n",
      "2020-09-21 22:35:50 iter = 41001\n",
      "2020-09-21 22:35:50 iter = 42001\n",
      "2020-09-21 22:35:53 iter = 43001\n",
      "2020-09-21 22:35:53 iter = 44001\n",
      "2020-09-21 22:35:53 iter = 45001\n",
      "2020-09-21 22:35:54 iter = 46001\n",
      "2020-09-21 22:35:54 iter = 47001\n",
      "2020-09-21 22:35:54 iter = 48001\n",
      "2020-09-21 22:35:55 iter = 49001\n",
      "2020-09-21 22:35:55 iter = 50001\n",
      "2020-09-21 22:35:55 iter = 51001\n",
      "2020-09-21 22:35:55 iter = 52001\n",
      "2020-09-21 22:35:56 iter = 53001\n",
      "2020-09-21 22:35:56 iter = 54001\n",
      "2020-09-21 22:35:56 iter = 55001\n",
      "2020-09-21 22:35:57 iter = 56001\n",
      "2020-09-21 22:35:57 iter = 57001\n",
      "2020-09-21 22:35:57 iter = 58001\n",
      "2020-09-21 22:35:57 iter = 59001\n",
      "2020-09-21 22:35:58 iter = 60001\n",
      "2020-09-21 22:35:58 iter = 61001\n",
      "2020-09-21 22:35:58 iter = 62001\n",
      "2020-09-21 22:35:58 iter = 63001\n",
      "2020-09-21 22:35:59 iter = 64001\n",
      "2020-09-21 22:35:59 iter = 65001\n",
      "2020-09-21 22:35:59 iter = 66001\n",
      "2020-09-21 22:36:01 iter = 67001\n",
      "2020-09-21 22:36:02 iter = 68001\n",
      "2020-09-21 22:36:02 iter = 69001\n",
      "2020-09-21 22:36:02 iter = 70001\n",
      "2020-09-21 22:36:03 iter = 71001\n",
      "2020-09-21 22:36:03 iter = 72001\n",
      "2020-09-21 22:36:03 iter = 73001\n",
      "2020-09-21 22:36:04 iter = 74001\n",
      "2020-09-21 22:36:04 iter = 75001\n",
      "2020-09-21 22:36:04 iter = 76001\n",
      "2020-09-21 22:36:04 iter = 77001\n",
      "2020-09-21 22:36:05 iter = 78001\n",
      "2020-09-21 22:36:05 iter = 79001\n",
      "2020-09-21 22:36:05 iter = 80001\n",
      "2020-09-21 22:36:06 iter = 81001\n",
      "2020-09-21 22:36:06 iter = 82001\n",
      "2020-09-21 22:36:06 iter = 83001\n",
      "2020-09-21 22:36:06 iter = 84001\n",
      "2020-09-21 22:36:07 iter = 85001\n",
      "2020-09-21 22:36:07 iter = 86001\n",
      "2020-09-21 22:36:07 iter = 87001\n",
      "2020-09-21 22:36:08 iter = 88001\n",
      "2020-09-21 22:36:08 iter = 89001\n",
      "2020-09-21 22:36:08 iter = 90001\n",
      "2020-09-21 22:36:08 iter = 91001\n",
      "2020-09-21 22:36:09 iter = 92001\n",
      "2020-09-21 22:36:09 iter = 93001\n",
      "2020-09-21 22:36:09 iter = 94001\n",
      "2020-09-21 22:36:10 iter = 95001\n",
      "2020-09-21 22:36:10 iter = 96001\n",
      "2020-09-21 22:36:12 iter = 97001\n",
      "2020-09-21 22:36:13 iter = 98001\n",
      "2020-09-21 22:36:13 iter = 99001\n",
      "2020-09-21 22:36:13 iter = 100001\n",
      "2020-09-21 22:36:13 iter = 101001\n",
      "2020-09-21 22:36:14 iter = 102001\n",
      "2020-09-21 22:36:14 iter = 103001\n",
      "2020-09-21 22:36:14 iter = 104001\n",
      "2020-09-21 22:36:15 iter = 105001\n",
      "2020-09-21 22:36:15 iter = 106001\n",
      "2020-09-21 22:36:15 iter = 107001\n",
      "2020-09-21 22:36:16 iter = 108001\n",
      "2020-09-21 22:36:16 iter = 109001\n",
      "2020-09-21 22:36:16 iter = 110001\n",
      "2020-09-21 22:36:17 iter = 111001\n",
      "2020-09-21 22:36:17 iter = 112001\n",
      "2020-09-21 22:36:17 iter = 113001\n",
      "2020-09-21 22:36:18 iter = 114001\n",
      "2020-09-21 22:36:18 iter = 115001\n",
      "2020-09-21 22:36:18 iter = 116001\n",
      "2020-09-21 22:36:19 iter = 117001\n",
      "2020-09-21 22:36:19 iter = 118001\n",
      "2020-09-21 22:36:19 iter = 119001\n",
      "2020-09-21 22:36:19 iter = 120001\n",
      "2020-09-21 22:36:20 iter = 121001\n",
      "2020-09-21 22:36:20 iter = 122001\n",
      "2020-09-21 22:36:20 iter = 123001\n",
      "2020-09-21 22:36:21 iter = 124001\n",
      "2020-09-21 22:36:21 iter = 125001\n",
      "2020-09-21 22:36:21 iter = 126001\n",
      "2020-09-21 22:36:21 iter = 127001\n"
     ]
    }
   ],
   "source": [
    "# 从文本中提取统计信息， 使用正则表达式从案件的文本中 提取涉及到的所有金额， 求出所有金额数据的统计信息\n",
    "# 包括：求和， 最大值， 最小值， 最大最小差值， 平均值， 标准差\n",
    "# TODO：案件文本的词的个数（长度）, 酒驾、毒品等关键词，日期、地点等关键词\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "train_raw = pd.read_csv('./data/train.csv', sep='\\t')\n",
    "train_raw = train_raw.reset_index(drop=True)\n",
    "\n",
    "amt_list = []\n",
    "for i, row in train_raw.iterrows():\n",
    "    if i % 1000 == 1:\n",
    "        log('iter = %d' % i)\n",
    "    amt = re.findall(u'(\\d*\\.?\\d+)元', row['text'])\n",
    "    amt_tt = re.findall(u'(\\d*\\.?\\d+)万元', row['text'])\n",
    "    for a in amt:\n",
    "        amt_list.append([row['ID'], float(a)])\n",
    "    for a in amt_tt:\n",
    "        amt_list.append([row['ID'], float(a) * 10000])\n",
    "amt_feat = pd.DataFrame(amt_list, columns=['ID', 'amount'])\n",
    "amt_feat = amt_feat.groupby('ID')['amount'].agg([sum, min, max, np.ptp, np.mean, np.std]).reset_index()\n",
    "amt_feat = pd.merge(train_raw, amt_feat, how='left', on='ID').drop(['ID', 'text'], axis=1)\n",
    "amt_feat.columns = ['amt_' + i for i in amt_feat.columns]\n",
    "\n",
    "amt_feat.to_csv('./data/amt.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "name": "main.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
