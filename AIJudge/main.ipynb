{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train_splited.csv'\n",
    "test_path = './data/test_splited.csv'\n",
    "import os\n",
    "nrows = None\n",
    "if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
    "    # 加载训练集和测试集\n",
    "    df_tr, df_te = preprocess.load_data(nrows=None)\n",
    "    \n",
    "    df_tr['penalty'] = df_tr['penalty'].parallel_apply(lambda x:x-1)\n",
    "    df_te['penalty'] = df_te['penalty'].parallel_apply(lambda x:x-1)\n",
    "    \n",
    "    df_tr.to_csv('./data/train_splited.csv', sep='\\t', index=False)\n",
    "    df_te.to_csv('./data/test_splited.csv', sep='\\t', index=False)\n",
    "else:\n",
    "    df_tr = pd.read_csv(train_path, sep='\\t', nrows=nrows)\n",
    "    df_te = pd.read_csv(test_path, sep='\\t', nrows=nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看列信息\n",
    "df_tr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练集和测试集结合起来\n",
    "df_all = pd.concat([df_tr, df_te])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义罚金类别的评估函数\n",
    "from sklearn.metrics import f1_score\n",
    "def micro_avg_f1(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从训练集和测试集中的文本训练\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 当设置为浮点数时，过滤出现在超过max_df/低于min_df比例的句子中的词语；正整数时,则是超过max_df句句子\n",
    "tfv = TfidfVectorizer(#analyzer='word',\n",
    "                      # analyzer是word时默认只匹配长度大于2的词，且自动屏蔽标点， 这回导致文本中的罚金1,000和小数 1.5被分开\n",
    "                      # \\w+ 这里能够取出单个的字 但仍然匹配不到小数 待优化 先跑通baseline\n",
    "                      token_pattern=r'(?u)\\b\\w+\\b',             \n",
    "                      ngram_range=(1,3),\n",
    "                      min_df=3, max_df=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用全部数据进行tfidf的转换 不合适 应当使用训练集的数据提取转换器\n",
    "# tfv.fit(df_all.text.values.tolist())\n",
    "tfv.fit(df_tr.text.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = tfv.transform(df_tr.text.values.tolist())      # 将输入文本转换为tf-idf表示形式\n",
    "train_y = df_tr.penalty.values\n",
    "# penalty_classes = len(df_tr.penalty.unique())            # 罚金类别种类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def log(stri):\n",
    "    now = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "    print(str(now) + ' ' + str(stri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def model_cv(model, skf, train_x, train_y):\n",
    "    penalty_classes = len(set(train_y))\n",
    "    stack = np.zeros((train_x.shape[0], penalty_classes))\n",
    "    score_va = 0\n",
    "\n",
    "    for i, (tr, va) in enumerate(skf.split(train_x, train_y)):\n",
    "        log('stack: %d/%d'%((i+1), n_folds))\n",
    "        model.fit(train_x[tr], train_y[tr])\n",
    "        predict_va = model.predict_proba(train_x[va])         # 划分出的验证集预测各类别概率\n",
    "        log('va acc:%f' % micro_avg_f1(train_y[va], model.predict(train_x[va])))         # 求出验证集的预测精度\n",
    "        score_va += micro_avg_f1(train_y[va], model.predict(train_x[va]))                # 验证集的f1-score， micro\n",
    "        stack[va] += predict_va\n",
    "\n",
    "    score_va /= n_folds\n",
    "    log('va avg acc:%f' % score_va)\n",
    "\n",
    "    return stack\n",
    "\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=2, n_jobs=-1, solver='sag', multi_class='ovr')\n",
    "stack_lr = model_cv(lr, skf, train_x, train_y)\n",
    "\n",
    "df_stack = pd.DataFrame(index=range(len(stack_lr)))\n",
    "for i in range(stack_lr.shape[1]):\n",
    "    df_stack['tfidf_lr_{}'.format(i)] = stack_lr[:, i]\n",
    "\n",
    "df_stack.to_csv('./data/lr_prob.csv', index=None, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "stack_bnb = model_cv(bnb, skf, train_x, train_y)\n",
    "df_stack = pd.DataFrame(index=range(len(stack_bnb)))\n",
    "for i in range(stack_bnb.shape[1]):\n",
    "    df_stack['tfidf_svc_{}'.format(i)] = stack_bnb[:, i]\n",
    "\n",
    "df_stack.to_csv('./data/bnb_prob.csv', index=None, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "stack_mnb = model_cv(mnb, skf, train_x, train_y)\n",
    "df_stack = pd.DataFrame(index=range(len(stack_mnb)))\n",
    "for i in range(stack_mnb.shape[1]):\n",
    "    df_stack['tfidf_mnb_{}'.format(i)] = stack_mnb[:, i]\n",
    "\n",
    "df_stack.to_csv('./data/mnb_prob.csv', index=None, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svc = svm.LinearSVC(loss='hinge', tol=0.000001, C=0.5, verbose=1, random_state=2020, max_iter=5000)\n",
    "classes = len(set(train_y))\n",
    "stack = np.zeros((train_x.shape[0], classes))\n",
    "score_va = 0\n",
    "\n",
    "for i, (tr, va) in enumerate(skf.split(train_x, train_y)):\n",
    "    log('stack: %d/%d'%((i+1), n_folds))\n",
    "    svc.fit(train_x[tr], train_y[tr])\n",
    "    predict_va = svc.decision_function(train_x[va])         # 划分出的验证集预测各类别概率\n",
    "    log('va acc:%f' % micro_avg_f1(train_y[va], svc.predict(train_x[va])))         # 求出验证集的预测精度\n",
    "    score_va += micro_avg_f1(train_y[va], svc.predict(train_x[va]))                # 验证集的f1-score， micro\n",
    "    stack[va] += predict_va\n",
    "\n",
    "score_va /= n_folds\n",
    "log('va avg acc:%f' % score_va)\n",
    "stack_svc = stack\n",
    "df_stack = pd.DataFrame(index=range(len(stack_svc)))\n",
    "for i in range(stack_svc.shape[1]):\n",
    "    df_stack['tfidf_svc_{}'.format(i)] = stack_svc[:, i]\n",
    "\n",
    "df_stack.to_csv('./data/svc_prob.csv', index=None, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文本中提取统计信息， 使用正则表达式从案件的文本中 提取涉及到的所有金额， 求出所有金额数据的统计信息\n",
    "# 包括：求和， 最大值， 最小值， 最大最小差值， 平均值， 标准差\n",
    "# TODO：案件文本的词的个数（长度）, 酒驾、毒品等关键词，日期、地点等关键词\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "train_raw = pd.read_csv('./data/train.csv', sep='\\t')\n",
    "train_raw = train_raw.reset_index(drop=True)\n",
    "\n",
    "amt_list = []\n",
    "for i, row in train_raw.iterrows():\n",
    "    if i % 1000 == 1:\n",
    "        log('iter = %d' % i)\n",
    "    amt = re.findall(u'(\\d*\\.?\\d+)元', row['text'])\n",
    "    amt_tt = re.findall(u'(\\d*\\.?\\d+)万元', row['text'])\n",
    "    for a in amt:\n",
    "        amt_list.append([row['ID'], float(a)])\n",
    "    for a in amt_tt:\n",
    "        amt_list.append([row['ID'], float(a) * 10000])\n",
    "amt_feat = pd.DataFrame(amt_list, columns=['ID', 'amount'])\n",
    "amt_feat = amt_feat.groupby('ID')['amount'].agg([sum, min, max, np.ptp, np.mean, np.std]).reset_index()\n",
    "amt_feat = pd.merge(train_raw, amt_feat, how='left', on='ID').drop(['ID', 'text'], axis=1)\n",
    "amt_feat.columns = ['amt_' + i for i in amt_feat.columns]\n",
    "\n",
    "amt_feat.to_csv('./data/amt.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "############################ 准备数据 ############################\n",
    "doc_f = open('./data/d2v.txt', 'w', encoding='utf8')\n",
    "for i, contents in enumerate(df_tr['text']):\n",
    "    if i % 10000 == 0:\n",
    "        log('iter = %d' % i)\n",
    "    doc_f.write(u'_*{} {}\\n'.format(i, contents))\n",
    "doc_f.close()\n",
    "\n",
    "class Doc_list(object):\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "    def __iter__(self):\n",
    "        for i, line in enumerate(codecs.open(self.f, encoding='utf-8')):\n",
    "            words = line.strip().split(' ')\n",
    "            tags = [int(words[0][2:])]\n",
    "            words = words[1:]\n",
    "            yield SentimentDocument(words, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Doc2vec\n",
    "from collections import namedtuple\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags')\n",
    "########################## PV-DBOW Doc2Vec #########################\n",
    "# 初始化Doc2Vec模型\n",
    "# @vector_size: Dimensionality of the feature vectors.\n",
    "# @window: The maximum distance between the current and predicted word within a sentence.\n",
    "# @alpha: The initial learning rate.\n",
    "# @dm: {1,0}, optional. Defines the training algorithm.\n",
    "#             If `dm=1`, 'distributed memory' (PV-DM) is used.\n",
    "#             Otherwise, `distributed bag of words` (PV-DBOW) is employed.\n",
    "# 一篇文档转化成300 x 1的向量\n",
    "d2v = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=3, window=30,\n",
    "              sample=1e-5, workers=4, alpha=0.025,\n",
    "              min_alpha=0.025\n",
    "             )\n",
    "# 数据生成器 Build vocabulary from a sequence of documents (can be a once-only generator stream).\n",
    "doc_list = Doc_list('./data/d2v.txt')\n",
    "d2v.build_vocab(doc_list)\n",
    "# 文档的标签\n",
    "df_lb = df_tr['penalty']\n",
    "\n",
    "for i in range(5):\n",
    "    log('pass: ' + str(i))\n",
    "    doc_list = Doc_list('./data/d2v.txt')\n",
    "    # 训练的doc2vec\n",
    "    d2v.train(doc_list, total_examples=d2v.corpus_count, epochs=d2v.epochs)\n",
    "    X_d2v = np.array([d2v.docvecs[i] for i in range(df_tr.shape[0])])\n",
    "    scores = cross_val_score(LogisticRegression(C=2, n_jobs=-1, solver='sag', multi_class='ovr'),\n",
    "                              X_d2v, df_lb,\n",
    "                              cv=5\n",
    "                             )\n",
    "    log('d2v-dbow: ' + str(scores) + ' ' + str(np.mean(scores)))\n",
    "d2v.save('./data/d2v-dbow.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## PV-DM Doc2Vec #########################\n",
    "\n",
    "d2v_dm = Doc2Vec(dm=1, vector_size=300, negative=5, hs=0, min_count=3, window=30,\n",
    "              sample=1e-5, workers=4, alpha=0.025,\n",
    "              min_alpha=0.025\n",
    "             )\n",
    "# 数据生成器 Build vocabulary from a sequence of documents (can be a once-only generator stream).\n",
    "doc_list = Doc_list('./data/d2v.txt')\n",
    "d2v_dm.build_vocab(doc_list)\n",
    "# 文档的标签\n",
    "df_lb = df_tr['penalty']\n",
    "\n",
    "for i in range(5):\n",
    "    log('pass: ' + str(i))\n",
    "    doc_list = Doc_list('./data/d2v.txt')\n",
    "    # 训练的doc2vec\n",
    "    d2v_dm.train(doc_list, total_examples=d2v_dm.corpus_count, epochs=d2v_dm.epochs)\n",
    "    X_d2v = np.array([d2v.docvecs[i] for i in range(df_tr.shape[0])])\n",
    "    scores = cross_val_score(LogisticRegression(C=2, n_jobs=-1, solver='sag', multi_class='ovr'),\n",
    "                              X_d2v, df_lb,\n",
    "                              cv=5\n",
    "                             )\n",
    "    log('doc2vec-dm: ' + str(scores) + ' ' + str(np.mean(scores)))\n",
    "d2v_dm.save('./data/d2v-dm.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用doc2vec 生成文档的向量特征 直接对向量特征使用逻辑回归拟合  平均得分为0.38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  使用神经网络对文本的Doc2Vec表示进行拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个模型\n",
    "from gensim.models import Doc2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "class Model():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def get_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(300, input_shape=(300,), activation='tanh'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(classes, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adadelta',\n",
    "                      metrics=['acc'],\n",
    "                     )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = Model().get_model()\n",
    "tmp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "def check_device():\n",
    "    # 查看有效的CPU和GPU\n",
    "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"99\"\n",
    "    print(device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "def assign_device():\n",
    "    # 指定使用GPU\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 选用GPU序号\n",
    "    config = ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = InteractiveSession(config=config)\n",
    "\n",
    "check_device()\n",
    "assign_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义keras的数据生成器\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, d2v_vectors, indexes, y, batch_size=32):\n",
    "        '''Initialization'''\n",
    "        self.d2v_vectors = d2v_vectors\n",
    "        self.indexes = indexes\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        ''' Denotes the number of batches per epoch'''\n",
    "        # 必须进行整型转换\n",
    "        return int(np.floor(len(self.indexes) / self.batch_size))\n",
    "\n",
    "\n",
    "    # 一个batch的数据处理，返回需要feed到模型中训练的数据\n",
    "    def __getitem__(self, index):\n",
    "        '''Generate one batch of data'''\n",
    "        # Generate indexes of the batch\n",
    "        indexes = [self.indexes[index] for index in range(index*self.batch_size, (index+1)*self.batch_size)]\n",
    "\n",
    "        # Get inputs and labels from original data\n",
    "        # 从模型中提取文本的向量化表示\n",
    "        x = [self.d2v_vectors[index] for index in indexes]\n",
    "        # 转成ndarray\n",
    "        x = np.array(x)\n",
    "        y = [self.y[index] for index in indexes]\n",
    "        y = np.array(y)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow import keras\n",
    "\n",
    "def d2v_nn(docvecs, y, feat, num_class, skf, batch_size, epochs, df_tr):\n",
    "\n",
    "    stack_d2v = np.zeros(y.shape)\n",
    "    score_va = 0\n",
    "\n",
    "    for i, (tr, va) in enumerate(skf.split(df_tr.text, df_tr.penalty)):\n",
    "        log('stack %s: %d/%d' % (feat, i+1, skf.get_n_splits()))\n",
    "\n",
    "        train_gen = DataGenerator(docvecs, tr, y, batch_size=batch_size)\n",
    "        va_gen = DataGenerator(docvecs, va, y, batch_size=batch_size)\n",
    "\n",
    "        model = Model().get_model()\n",
    "        callbacks_list = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_acc',\n",
    "                patience=2,\n",
    "                mode='max'\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath= feat + '.h5',\n",
    "                monitor='val_acc',\n",
    "                save_best_only=True,\n",
    "            )]\n",
    "        history = model.fit(train_gen,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=2,\n",
    "                            validation_data=va_gen,\n",
    "                            callbacks=callbacks_list\n",
    "                           )\n",
    "\n",
    "        y_pred_va = model.predict(va_gen)\n",
    "        print(y_pred_va.shape)\n",
    "        log('va acc: %f' % micro_avg_f1(df_tr.penalty[va], np.argmax(model.predict(va_gen), axis=-1)))\n",
    "        score_va += micro_avg_f1(df_tr.penalty[va], np.argmax(model.predict(va_gen), axis=-1))\n",
    "        stack_d2v[va] += y_pred_va\n",
    "\n",
    "    score_va /= n\n",
    "    log('va avg acc: %f' % score_va)\n",
    "    for l in range(stack_d2v.shape[1]):\n",
    "        df_stack['{}_{}'.format(feat, l)] = stack_d2v[:, l]\n",
    "\n",
    "    return df_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "epochs = 1000\n",
    "n = 5\n",
    "\n",
    "feat = 'd2v_dbow'\n",
    "\n",
    "# 加载训练完成的Doc2Vec-dbow模型\n",
    "d2v_dbow = Doc2Vec.load('./data/d2v-dbow.model')\n",
    "dbow_docvecs = d2v_dbow.docvecs\n",
    "y = to_categorical(df_tr.penalty, classes)\n",
    "\n",
    "stack_d2v_dbow = d2v_nn(dbow_docvecs, y, feat, classes, skf, batch_size, epochs, df_tr)\n",
    "stack_d2v_dbow.to_csv('./data/d2v-dbow.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练完成的Doc2Vec-dbow模型\n",
    "d2v_dbow = Doc2Vec.load('./data/d2v-dm.model')\n",
    "dbow_docvecs = d2v_dbow.docvecs\n",
    "y = to_categorical(df_tr.penalty, classes)\n",
    "\n",
    "stack_d2v_dbow = d2v_nn(dbow_docvecs, y, feat, classes, skf, batch_size, epochs, df_tr)\n",
    "stack_d2v_dbow.to_csv('./data/d2v-dm.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_d2v_dbow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练word2vec词向量\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "\n",
    "documents = df_tr.text.values\n",
    "texts = [[word for word in document.split(' ')] for document in documents]\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] >= 5] for text in texts]\n",
    "\n",
    "log('Train Model...')\n",
    "w2v = Word2Vec(texts, size=100, window=5, iter=15, seed=2020)\n",
    "log('Done')\n",
    "w2v.save('./data/w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对一篇文档中词的Word2vec向量求和再求均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_feat = np.zeros((len(texts), 100))\n",
    "w2v_feat_avg = np.zeros((len(texts), 100))\n",
    "i = 0\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        vec = w2v[token]\n",
    "        w2v_feat[i, :] += vec\n",
    "    w2v_feat_avg[i, :] = w2v_feat[i, :] / len(text)\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        log('Vectorizing document with W2V %d' % i)\n",
    "        \n",
    "df_w2v = pd.DataFrame(w2v_feat)\n",
    "df_w2v.columns = ['w2v_' + str(i) for i in df_w2v.columns]\n",
    "df_w2v.to_csv('./data/w2v_feat.csv', encoding='utf8', index=None)\n",
    "df_w2v_avg = pd.DataFrame(w2v_feat_avg)\n",
    "df_w2v_avg.columns = ['w2v_avg_' + str(i) for i in df_w2v_avg.columns]\n",
    "df_w2v_avg.to_csv('./data/w2v_avg_feat.csv', encoding='utf8', index=None)\n",
    "\n",
    "log('Save w2v and w2v_avg feat done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_lr = pd.read_csv('./data/lr_prob.csv')\n",
    "tfidf_bnb = pd.read_csv('./data/bnb_prob.csv')\n",
    "tfidf_mnb = pd.read_csv('./data/mnb_prob.csv')\n",
    "tfidf_svc = pd.read_csv('./data/svc_prob.csv')\n",
    "amt = pd.read_csv('./data/amt.csv')\n",
    "amt = amt.drop(['amt_items'], axis=1)\n",
    "dbow_nn = pd.read_csv('./data/d2v-dbow.csv', sep='\\t')\n",
    "dm_nn = pd.read_csv('./data/d2v-dm.csv', sep='\\t')\n",
    "w2v_sum = pd.read_csv('./data/w2v_feat.csv')\n",
    "w2v_avg = pd.read_csv('./data/w2v_avg_feat.csv')\n",
    "\n",
    "df = pd.concat([tfidf_lr, tfidf_bnb, tfidf_mnb,\n",
    "                tfidf_svc, tfidf_svc, amt,\n",
    "                dbow_nn, dm_nn, w2v_sum,\n",
    "                w2v_avg], axis=1\n",
    "              )\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 7\n",
    "min_child_weight = 1\n",
    "subsample = 0.8\n",
    "colsample_bytree = 0.8\n",
    "gamma = 1\n",
    "lam = 0\n",
    "\n",
    "n_trees = 10000\n",
    "esr = 200\n",
    "evals = 20\n",
    "\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'booster': 'gbtree',\n",
    "    'stratified': True,\n",
    "    'num_class': num_class,\n",
    "    'max_depth': max_depth,\n",
    "    'min_child_weight': min_child_weight,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "#     'gamma': gamma,\n",
    "#     'lambda': lam,\n",
    "\n",
    "    'eta': 0.02,\n",
    "    'silent': 1,\n",
    "    'seed': seed,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df_tr.penalty.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need debug\n",
    "\n",
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(train_x, train_y)\n",
    "dvalid = xgb.DMatrix(test_x, test_y)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'test')]\n",
    "bst = xgb.train(params, dtrain, n_trees, evals=watchlist, feval=micro_avg_f1, maximize=True,\n",
    "                early_stopping_rounds=esr, verbose_eval=evals)\n",
    "y_pred = bst.predict(dvalid).astype(int)\n",
    "\n",
    "print(micro_avg_f1(test_y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "name": "main.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
