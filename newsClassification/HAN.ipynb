{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train, df_test = utils.read_data(test_data='./data/test_b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = utils.split_train_val(df_train,\n",
    "                                                       test_size=0.2,\n",
    "                                                       random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.assign_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "#from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.layers import Layer\n",
    "#from keras.engine.topology import Layer\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence = Attention()(hidden)\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'a':3}\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义一共有多少个字 或者词 或者特征\n",
    "max_features = 8000\n",
    "# 定义一个篇文本，最多有多少个句子\n",
    "maxlen_text = 20\n",
    "# 定义一个句子， 最多有多少个词\n",
    "maxlen_sentence = 25\n",
    "# 定义一共有多少个类别\n",
    "n_classes = 14\n",
    "# 词嵌入的维度\n",
    "embedding_dims = 30\n",
    "\n",
    "filters = 32\n",
    "kernel_size = 7\n",
    "\n",
    "batch_size = 200\n",
    "epochs = 100\n",
    "\n",
    "train_generator = utils.DataGeneratorHAN(x_train, y_train,\n",
    "                                    n_classes,\n",
    "                                    batch_size=batch_size,\n",
    "                                    maxlen_text=maxlen_text,\n",
    "                                    maxlen_sentence=maxlen_sentence,\n",
    "                                    )\n",
    "\n",
    "val_generator = utils.DataGeneratorHAN(x_val, y_val,\n",
    "                                  n_classes,\n",
    "                                  batch_size=batch_size,\n",
    "                                  maxlen_text=maxlen_text,\n",
    "                                  maxlen_sentence=maxlen_sentence,\n",
    "                                  )\n",
    "test_generator = utils.DataGeneratorHAN(df_test.text.values.tolist(),\n",
    "                                        batch_size=100,\n",
    "                                        maxlen_text=maxlen_text,\n",
    "                                        maxlen_sentence=maxlen_sentence,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, Conv1D\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "class HAN():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_model(self):\n",
    "        input_words = Input(shape=(maxlen_sentence,))\n",
    "        x_words = Embedding(max_features, embedding_dims,\n",
    "                            input_length=maxlen_sentence)(input_words)\n",
    "        x_words = Conv1D(maxlen_sentence, 7, activation='relu')(x_words)\n",
    "        x_words = Bidirectional(LSTM(256, return_sequences=True,\n",
    "                                     activation='tanh',\n",
    "                                     recurrent_activation='sigmoid',\n",
    "                                     dropout=0.1,\n",
    "                                     recurrent_dropout=0))(x_words)\n",
    "        x_words = Attention(19)(x_words)\n",
    "        model_words = Model(input_words, x_words)\n",
    "\n",
    "        # Sentence part\n",
    "        input_sentences = Input(shape=(maxlen_text, maxlen_sentence))\n",
    "        x_sentence = TimeDistributed(model_words)(input_sentences)\n",
    "        x_sentence = Bidirectional(LSTM(256, return_sequences=True,\n",
    "                                        activation='tanh',\n",
    "                                        recurrent_activation='sigmoid',\n",
    "                                        dropout=0.1,\n",
    "                                        recurrent_dropout=0))(x_sentence)\n",
    "        x_sentence = Attention(maxlen_text)(x_sentence)\n",
    "        output = Dense(n_classes, activation='softmax')(x_sentence)\n",
    "        model = Model(inputs=input_sentences, outputs=output)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'a':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_f1_score',\n",
    "        factor=0.1,\n",
    "        patience=3,\n",
    "        mode='max',\n",
    "        cooldown=3\n",
    "        ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_f1_score',\n",
    "        patience=10,\n",
    "        mode='max'\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='han_weights.h5',\n",
    "        monitor='val_f1_score',\n",
    "        save_best_only=True,\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "han = HAN().get_model()\n",
    "\n",
    "han.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[utils.F1_score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = han.fit(train_generator,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  validation_data=val_generator,\n",
    "                  validation_freq=1,\n",
    "                  callbacks=callbacks_list,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.do_predict(han, test_generator, './data/submit_han_b.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/home/yahu/anaconda3/envs/ml/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "HAN.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
