{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 定义问题\n",
    "- 有哪些数据可以用？\n",
    "- 想要预测什么？\n",
    "- 是否需要收集更多数据或雇人为数据手动添加标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "数据为新闻文本，并按照字符级别进行匿名处理。整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐的文本数据。\n",
    "赛题数据由以下几个部分构成：训练集20w条样本，测试集A包括5w条样本，测试集B包括5w条样本。为了预防选手人工标注测试集的情况，我们将比赛数据的文本按照字符级别进行了匿名处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 若文件中存在utf-8不能解码的内容， 使用unicode_escape编码格式\n",
    "df = pd.read_csv('data/train_set.csv', encoding='utf-8', sep='\\t', nrows=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看数据\n",
    "print(df.columns)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 统计每个句子的长度\n",
    "df['text_len'] = df['text'].apply(lambda x: len(x.split(' ')))\n",
    "# print(df['text_len'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取测试集\n",
    "\n",
    "df_test = pd.read_csv('data/test_a.csv', encoding='utf-8', sep='\\t', nrows=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#　收集每条测试文本的长度信息\n",
    "df_test['text_len'] = df_test['text'].apply(lambda x: len(x.split(' ')))\n",
    "# print(df_test['text_len'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "原始数据有两列，第一列为标签，第二列为进行匿名处理的文本, 手动添加一列，记录每条文本的长度。一共有20万条文本，最大长度为57921，最短长度为2。每条记录的长度差异较大， 需要对较大的进行截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看新闻类别\n",
    "print(df['label'].unique())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 一共有14个类别，查看各个类别分布\n",
    "import matplotlib.pyplot as plt\n",
    "df['label'].value_counts().plot(kind='bar')\n",
    "ax = plt.gca() # 获取当前的axes\n",
    "ax.spines['left'].set_color('red')\n",
    "ax.spines['bottom'].set_color('red')\n",
    "plt.title('News class count')\n",
    "plt.xlabel('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = df.loc[df['text_len']>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 尝试删除长度过小的文本，测试集中长度最小的文本为14, 只有0.25的文本长度小于370\n",
    "# 查看长度小于10的文本的类别分布情况\n",
    "print(tmp['text_len'].describe())\n",
    "print(tmp.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "tmp['label'].value_counts().plot(kind='bar')\n",
    "ax = plt.gca() # 获取当前的axes\n",
    "ax.spines['left'].set_color('red')\n",
    "ax.spines['bottom'].set_color('red')\n",
    "plt.title('News class count')\n",
    "plt.xlabel('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "各个类别分布严重不平衡, 且删除长度小于10的文本后，各类别分布基本不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 使用长度不小于10的文本，作为训练集\n",
    "df = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 统计每个字符出现的次数\n",
    "# 执行时 kernel will restart\n",
    "if False:\n",
    "    from collections import Counter\n",
    "    all_lines = ' '.join(list(df['text']))\n",
    "    word_count = Counter(all_lines.split(\" \"))\n",
    "    word_count = sorted(word_count.items(), key=lambda d:d[1], reverse=True)\n",
    "    # 一共有多少个字\n",
    "    print(len(word_count))\n",
    "    # 出现次数最多的字的编号\n",
    "    print(word_count[0])\n",
    "    # 出现次数最少的字的编号\n",
    "    print(word_count[-1])\n",
    "# 根据不同字符在句子中出现的次数， 推测标点符号\n",
    "# 根据推测的标点符号， 分析每篇新闻由多少个句子组成\n",
    "# 分析每类新闻中 出现次数最多的字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看训练集中最大的字的编号\n",
    "df['max'] = df['text'].apply(lambda x: max([int(num) for num in x.split()]))\n",
    "df['max'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看训练集中最小的字的编号\n",
    "df['max'] = df['text'].apply(lambda x: min([int(num) for num in x.split()]))\n",
    "df['max'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据分析的结论\n",
    "1. 每个新闻平均字符个数较多，可能需要截断\n",
    "2. 各个类别不均衡， 会严重影响模型的精度\n",
    "3. 训练集中最大的编号为7549， 假设共有10000个不同的编号，即max_features=10000\n",
    "4. 设置文本的长度为最大长度为300， maxlen=300\n",
    "5. 最小编号为0， padding之前应对所有字符+1 或指定padding的value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 评估目标的方法\n",
    "- 使用哪种指标对目标进行评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "本项目共存在14个类别， 且类别分布严重不平衡， 所以采用f1-score作为评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 准备用于评估模型的验证过程。\n",
    "- 定义训练集、验证集和测试集。验证集和测试集应该和训练集分开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 划分训练集和验证集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train = df['text'].values.tolist()\n",
    "y_train = df['label'].values.tolist()\n",
    "x_test = df_test['text'].values.tolist()\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train,\n",
    "                                                  y_train,\n",
    "                                                  test_size=0.3,\n",
    "                                                  random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import DataGenerator\n",
    "from utils import DataGeneratorHAN\n",
    "from utils import F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.assign_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义一个序列的最大长度\n",
    "maxlen = 400\n",
    "n_classes = 14\n",
    "# 定义最大的字的编号（特征数）\n",
    "max_features = 8000\n",
    "batch_size = 200\n",
    "epochs = 100\n",
    "embedding_dims = 128\n",
    "\n",
    "train_generator = DataGenerator(x_train, y_train,\n",
    "                                n_classes,\n",
    "                                batch_size=batch_size,\n",
    "                                maxlen=maxlen,\n",
    "                               )\n",
    "val_generator = DataGenerator(x_val, y_val,\n",
    "                              n_classes,\n",
    "                              batch_size=batch_size,\n",
    "                              maxlen=maxlen,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 数据向量化（数据预处理）\n",
    "- 将数据转换为能被神经网络接收的形式 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 导入必须的包\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Embedding, SimpleRNN\n",
    "from utils import F1_score\n",
    "\n",
    "# 定义一个简单的RNN模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 100))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=[F1_score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow import keras\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_f1_score',\n",
    "        patience=2,\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='SimpleRNN.h5',\n",
    "        monitor='val_f1_score',\n",
    "        save_best_only=True,\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 拟合模型\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_freq=1,\n",
    "                    callbacks=callbacks_list,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 开发模型\n",
    "- 使用fasttext模型作为基线模型， fasttext在划分的验证集上的f1score为0.8972\n",
    "- 简单的RNN模型验证的f1-score为0.744远小于 fasttext，\n",
    "- 尝试使用biLSTM模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 调节超参数和正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 使用一个biLSTM提取特征\n",
    "# 导入必须的包\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Embedding, Bidirectional, LSTM\n",
    "\n",
    "# 定义一个简单的双向RNN模型\n",
    "class biLSTM():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(max_features, 100))\n",
    "        # model.add(SimpleRNN(32))\n",
    "        model.add(Bidirectional(LSTM(128)))\n",
    "        model.add(Dense(n_classes, activation='softmax'))\n",
    "        model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "                      metrics=[F1_score()])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_f1_score',\n",
    "        patience=2,\n",
    "        mode='max'\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='biLSTM.h5',\n",
    "        monitor='val_f1_score',\n",
    "        save_best_only=True,\n",
    "    )]\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_freq=1,\n",
    "                    callbacks=callbacks_list,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "双向LSTM模型在训练到第4轮时达到最佳f1-score： 0.8367，相比于简单的RNN有了较大的提升， 但仍远小于fasttext的效果。\n",
    "从之前类别分布的信息中，我们知道， 样本的类别分布是非常不平衡的， 但是对于每个类别， 我们是同等对待的。 因此这里引入样本权重，来解决样本分布不均衡的问题。\n",
    "在生成器中， 我们没有加入样本的权重， 下面可以尝试添加样本的权重再次训练该网络。为了避免将验证集的信息引入模型的训练过程， 在计算类别权重时，应该使用划分好的训练数据。\n",
    "本问题中，各个类别之间应该是同等重要的，因此，不指定类别权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义一个序列的最大长度\n",
    "maxlen = 400\n",
    "n_classes = 14\n",
    "# 定义最大的字的编号（特征数）\n",
    "max_features = 8000\n",
    "batch_size = 1024\n",
    "epochs = 100\n",
    "embedding_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(x_train, y_train,\n",
    "                                n_classes,\n",
    "                                batch_size=batch_size,\n",
    "                                maxlen=maxlen,\n",
    "                               )\n",
    "val_generator = DataGenerator(x_val, y_val,\n",
    "                              n_classes,\n",
    "                              batch_size=batch_size,\n",
    "                              maxlen=maxlen,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_f1_score',\n",
    "        patience=2,\n",
    "        mode='max'\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='biLSTM_sample_weights.h5',\n",
    "        monitor='val_f1_score',\n",
    "        save_best_only=True,\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = biLSTM().get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_freq=1,\n",
    "                    callbacks=callbacks_list,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "引入样本权重后， 模型泛化效果并没有想象中的得到提升，f1-score仅有0.8394， 下面定义一个卷积网络进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "class TextCNN():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "        model.add(Conv1D(32, 7, activation='relu'))\n",
    "        model.add(MaxPooling1D(5))\n",
    "        model.add(Conv1D(32, 7, activation='relu'))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(Dense(n_classes, activation='softmax'))\n",
    "        return model\n",
    "\n",
    "model = TextCNN().get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[F1_score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_freq=1,\n",
    "                    callbacks=callbacks_list,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Epoch 8/10  \n",
    "136/136 [==============================] - ETA: 0s - loss: 0.2736 - f1_score_1: 0.8938WARNING:tensorflow:Early   stopping conditioned on metric `val_f1_score` which is not available. Available metrics are:   loss,f1_score_1,val_loss,val_f1_score_1  \n",
    "WARNING:tensorflow:Can save best model only with val_f1_score available, skipping.  \n",
    "136/136 [==============================] - 43s 313ms/step - loss: 0.2736 - f1_score_1: 0.8938 - val_loss: 0.4793 - val_f1_score_1: 0.8729  \n",
    "同一个notebook中第二次拟合模型时， monitor metrics 会由定义的val_f1_score变成val_f1_score_1  \n",
    "TextCNN在训练到第8轮时达到最优， 然后开始过拟合， f1-score为0.8729， 相比biLSTM有了些许提升， 相比fasttext的0.89已非常接近。  \n",
    "尝试结合RNN和CNN进行训练  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN, Lambda, Concatenate, Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "\n",
    "class TextRCNN(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_model(self):\n",
    "        input_text = Input((maxlen,))\n",
    "\n",
    "        embedder = Embedding(max_features, embedding_dims, input_length=maxlen)\n",
    "        embedding = embedder(input_text)\n",
    "\n",
    "        x_left = SimpleRNN(128, return_sequences=True)(embedding)\n",
    "        x_right = SimpleRNN(128, return_sequences=True, go_backwards=True)(embedding)\n",
    "        x_right = Lambda(lambda x: K.reverse(x, axes=1))(x_right)\n",
    "        x = Concatenate(axis=2)([x_left, embedding, x_right])\n",
    "\n",
    "        x = Conv1D(64, kernel_size=1, activation='tanh')(x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "        output = Dense(n_classes, activation='softmax')(x)\n",
    "        model = Model(inputs=input_text, outputs=output)\n",
    "        return model\n",
    "    \n",
    "textRCNN = TextRCNN().get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textRCNN.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[F1_score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_f1_score',\n",
    "        patience=2,\n",
    "        mode='max'\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='RCNN.h5',\n",
    "        monitor='val_f1_score',\n",
    "        save_best_only=True,\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = textRCNN.fit(train_generator,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_freq=1,\n",
    "                    callbacks=callbacks_list,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "RCNN的f1-score最高达到了0.90, 超过了fastText, 且在第10轮达到最佳，第11轮开始过拟合， 使用该模型提交测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_text = df_test.text.values.tolist()\n",
    "test_generator = DataGenerator(test_text,\n",
    "                                batch_size=100,\n",
    "                                maxlen=maxlen,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = textRCNN.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = np.argmax(result, axis=1)\n",
    "result = pd.DataFrame({'label': result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.to_csv('rcnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('./data/test_a_sample_submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_generator = DataGenerator(x_train, y_train,\n",
    "                                   batch_size=batch_size,\n",
    "                                   maxlen=maxlen,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 在所有训练集上进行一DataGeneratorl_train_generator = DataGenerator(x_train, y_train, batch_size=batch_size, maxlen=maxlen)\n",
    "textRCNN_from_scratch = TextRCNN().get_model()\n",
    "textRCNN_from_scratch.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[F1_score()])\n",
    "history = textRCNN_from_scratch.fit(all_train_generator,\n",
    "                    epochs=10,\n",
    "                    batch_size=100,\n",
    "                   )\n",
    "result_from_scratch = textRCNN_from_scratch.predict(test_generator)\n",
    "result_from_scratch = np.argmax(result_from_scratch, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_from_scratch = pd.DataFrame({'label': result_from_scratch})\n",
    "result.to_csv('rcnn_from_scratch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "#from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.layers import Layer\n",
    "#from keras.engine.topology import Layer\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence = Attention()(hidden)\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义一个序列的最大长度\n",
    "maxlen = 400\n",
    "n_classes = 14\n",
    "# 定义最大的字的编号（特征数）\n",
    "max_features = 8000\n",
    "maxlen_text = 16\n",
    "maxlen_sentence = 25\n",
    "batch_size = 200\n",
    "epochs = 100\n",
    "embedding_dims = 128\n",
    "\n",
    "train_generator = DataGeneratorHAN(x_train, y_train,\n",
    "                                n_classes,\n",
    "                                batch_size=batch_size,\n",
    "                                maxlen_text=maxlen_text,\n",
    "                                   maxlen_sentence=maxlen_sentence,\n",
    "                               )\n",
    "val_generator = DataGeneratorHAN(x_val, y_val,\n",
    "                              n_classes,\n",
    "                              batch_size=batch_size,\n",
    "                                 maxlen_text=maxlen_text,\n",
    "                                 maxlen_sentence=maxlen_sentence,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "class HAN():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_model(self):\n",
    "        input_words = Input(shape=(maxlen_sentence,))\n",
    "        x_words = Embedding(max_features, embedding_dims,\n",
    "                            input_length=maxlen_sentence)(input_words)\n",
    "        x_words = Bidirectional(LSTM(128, return_sequences=True))(x_words)\n",
    "        x_words = Attention(maxlen_sentence)(x_words)\n",
    "        model_words = Model(input_words, x_words)\n",
    "        \n",
    "        # Sentence part\n",
    "        input_sentences = Input(shape=(maxlen_text, maxlen_sentence))\n",
    "        x_sentence = TimeDistributed(model_words)(input_sentences)\n",
    "        x_sentence = Bidirectional(LSTM(128, return_sequences=True))(x_sentence)\n",
    "        x_sentence = Attention(maxlen_text)(x_sentence)\n",
    "        \n",
    "        output = Dense(n_classes, activation='softmax')(x_sentence)\n",
    "        model = Model(inputs=input_sentences, outputs=output)\n",
    "        \n",
    "        return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "han = HAN().get_model()\n",
    "\n",
    "han.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[F1_score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='f1_score',\n",
    "        patience=2,\n",
    "        mode='max'\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='han_weights.h5',\n",
    "        monitor='f1_score',\n",
    "        save_best_only=True,\n",
    "    )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = han.fit(train_generator,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_freq=1,\n",
    "                    callbacks=callbacks_list,\n",
    "                   )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/home/yahu/anaconda3/envs/ml/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "name": "deeplearning_method.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
