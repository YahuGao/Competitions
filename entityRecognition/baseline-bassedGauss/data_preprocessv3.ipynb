{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 句子切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cut(sentence):\n",
    "    \"\"\"\n",
    "    将一段文本切分成多个句子\n",
    "    :param sentence:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_sentence = []\n",
    "    sen = []\n",
    "    for i in sentence:\n",
    "        if i in ['。', '！', '？', '?'] and len(sen) != 0:\n",
    "            sen.append(i)\n",
    "            new_sentence.append(\"\".join(sen))\n",
    "            sen = []\n",
    "            continue\n",
    "        sen.append(i)\n",
    "\n",
    "    if len(new_sentence) <= 1: # 一句话超过max_seq_length且没有句号的，用\",\"分割，再长的不考虑了。\n",
    "        new_sentence = []\n",
    "        sen = []\n",
    "        for i in sentence:\n",
    "            if i.split(' ')[0] in ['，', ','] and len(sen) != 0:\n",
    "                sen.append(i)\n",
    "                new_sentence.append(\"\".join(sen))\n",
    "                sen = []\n",
    "                continue\n",
    "            sen.append(i)\n",
    "    if len(sen) > 0:  # 若最后一句话无结尾标点，则加入这句话\n",
    "        new_sentence.append(\"\".join(sen))\n",
    "    return new_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_test_set(text_list,len_treshold):\n",
    "    cut_text_list = []\n",
    "    cut_index_list = []\n",
    "    for text in text_list:\n",
    "\n",
    "        temp_cut_text_list = []\n",
    "        text_agg = ''\n",
    "        if len(text) < len_treshold:\n",
    "            temp_cut_text_list.append(text)\n",
    "        else:\n",
    "            sentence_list = _cut(text)  # 一条数据被切分成多句话\n",
    "            for sentence in sentence_list:\n",
    "                if len(text_agg) + len(sentence) < len_treshold:\n",
    "                    text_agg += sentence\n",
    "                else:\n",
    "                    temp_cut_text_list.append(text_agg)\n",
    "                    text_agg = sentence\n",
    "            temp_cut_text_list.append(text_agg)  # 加上最后一个句子\n",
    "\n",
    "        cut_index_list.append(len(temp_cut_text_list))\n",
    "        cut_text_list += temp_cut_text_list\n",
    "\n",
    "    return cut_text_list, cut_index_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置样本长度\n",
    "text_length = 250\n",
    "def from_ann2dic(r_ann_path, r_txt_path, w_path, w_file):\n",
    "    q_dic = {}\n",
    "    with codecs.open(r_ann_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip(\"\\n\\r\")\n",
    "            line_arr = line.split('\\t')\n",
    "            entityinfo = line_arr[1]\n",
    "            entityinfo = entityinfo.split(' ')\n",
    "            cls = entityinfo[0]\n",
    "            start_index = int(entityinfo[1])\n",
    "            end_index = int(entityinfo[2])\n",
    "            length = end_index - start_index\n",
    "            for r in range(length):\n",
    "                if r == 0:\n",
    "                    q_dic[start_index] = (\"B-%s\" % cls)\n",
    "                else:\n",
    "                    q_dic[start_index + r] = (\"I-%s\" % cls)\n",
    "\n",
    "    with codecs.open(r_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content_str = f.read()\n",
    "        \n",
    "    \n",
    "    cut_text_list, cut_index_list = cut_test_set([content_str],text_length)\n",
    "    \n",
    "    i = 0\n",
    "    for idx, line in enumerate(cut_text_list):\n",
    "        w_path_ = \"%s/%s-%s-new.txt\" % (w_path, w_file,idx)\n",
    "        with codecs.open(w_path_, \"w\", encoding=\"utf-8\") as w:\n",
    "            for str_ in line:\n",
    "                if str_ is \" \" or str_ == \"\" or str_ == \"\\n\" or str_ == \"\\r\":\n",
    "                    pass\n",
    "                else:\n",
    "                    if i in q_dic:\n",
    "                        tag = q_dic[i]\n",
    "                    else:\n",
    "                        tag = \"O\"  # 大写字母O\n",
    "                    w.write('%s %s\\n' % (str_, tag))\n",
    "                i+=1\n",
    "            w.write('%s\\n' % \"END O\")            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 划分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "file_list = glob.glob('./round1_train/train/*.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filelist, val_filelist = train_test_split(file_list,test_size=0.2,random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir  ./round1_train/train_new/\n",
    "!mkdir ./round1_train/val_new/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "data_dir = './round1_train/train/'\n",
    "for file in train_filelist:\n",
    "    if file.find(\".ann\") == -1 and file.find(\".txt\") == -1:\n",
    "        continue\n",
    "    file_name = file.split('/')[-1].split('.')[0]\n",
    "    r_ann_path = os.path.join(data_dir, \"%s.ann\" % file_name)\n",
    "    r_txt_path = os.path.join(data_dir, \"%s.txt\" % file_name)\n",
    "    w_path = './round1_train/train_new/'\n",
    "    w_file = file_name\n",
    "    from_ann2dic(r_ann_path, r_txt_path, w_path,w_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "data_dir = './round1_train/train/'\n",
    "for file in val_filelist:\n",
    "    if file.find(\".ann\") == -1 and file.find(\".txt\") == -1:\n",
    "        continue\n",
    "    file_name = file.split('/')[-1].split('.')[0]\n",
    "    r_ann_path = os.path.join(data_dir, \"%s.ann\" % file_name)\n",
    "    r_txt_path = os.path.join(data_dir, \"%s.txt\" % file_name)\n",
    "    w_path = './round1_train/val_new/'\n",
    "    w_file = file_name\n",
    "    from_ann2dic(r_ann_path, r_txt_path, w_path,w_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练集合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_path = \"./round1_train/data/train.txt\"\n",
    "for file in os.listdir('./round1_train/train_new/'):\n",
    "    path = os.path.join(\"./round1_train/train_new\", file)\n",
    "    if not file.endswith(\".txt\"):  \n",
    "        continue\n",
    "    q_list = []\n",
    "    print(\"开始读取文件:%s\" % file)\n",
    "    with codecs.open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        line = f.readline()\n",
    "        line = line.strip(\"\\n\\r\")\n",
    "        while line != \"END O\":\n",
    "            q_list.append(line)\n",
    "            line = f.readline()\n",
    "            line = line.strip(\"\\n\\r\")\n",
    "    print(\"开始写入文本%s\" % w_path)\n",
    "    with codecs.open(w_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for item in q_list:\n",
    "            if item.__contains__('\\ufeff1'):\n",
    "                print(\"===============\")\n",
    "            f.write('%s\\n' % item)\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证集合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_path = \"./round1_train/data/val.txt\"\n",
    "for file in os.listdir('./round1_train/val_new/'):\n",
    "    path = os.path.join(\"./round1_train/val_new\", file)\n",
    "    if not file.endswith(\".txt\"):  \n",
    "        continue\n",
    "    q_list = []\n",
    "\n",
    "    with codecs.open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        line = f.readline()\n",
    "        line = line.strip(\"\\n\\r\")\n",
    "        while line != \"END O\":\n",
    "            q_list.append(line)\n",
    "            line = f.readline()\n",
    "            line = line.strip(\"\\n\\r\")\n",
    "    \n",
    "    with codecs.open(w_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for item in q_list:\n",
    "            if item.__contains__('\\ufeff1'):\n",
    "                print(\"===============\")\n",
    "            f.write('%s\\n' % item)\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原始验证集拷贝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in val_filelist:\n",
    "    file_name = file.split('/')[-1].split('.')[0]\n",
    "    r_ann_path = os.path.join(\"./round1_train/train\", \"%s.ann\" % file_name)\n",
    "    os.system(\"cp %s %s\"%(file,\"./round1_train/val_data\"))\n",
    "    os.system(\"cp %s %s\"%(r_ann_path,\"./round1_train/val_data\"))\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
