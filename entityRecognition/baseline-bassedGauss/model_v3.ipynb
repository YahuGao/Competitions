{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试了学习率 e-5, e-4, e-3和动态学习率（每轮学习率减半）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WCZGSwO08_ji",
    "outputId": "521d05e0-78dc-4a8f-c5f2-0f7528b1c420"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "pc13awwc9Sxk",
    "outputId": "10e28168-dd15-4291-94e0-8c038a4250e1"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.insert(0,'/notebook/.custom/TF2.1.0_JUPYTER2_gpu/pylib/Python3')\n",
    "import bert4keras\n",
    "bert4keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "967twa9v6LWa"
   },
   "outputs": [],
   "source": [
    "#! -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "from bert4keras.backend import keras, K\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.optimizers import Adam\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from bert4keras.snippets import open, ViterbiDecoder, to_array\n",
    "from bert4keras.layers import ConditionalRandomField\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4U1aW14R5NbM"
   },
   "outputs": [],
   "source": [
    "maxlen = 250\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "bert_layers = 12\n",
    "learing_rate = 1e-4  # bert_layers越小，学习率应该要越大\n",
    "crf_lr_multiplier = 1000  # 必要时扩大CRF层的学习率\n",
    "\n",
    "# bert配置\n",
    "config_path = '/data/bert/bert_config.json'\n",
    "checkpoint_path = '/data/bert/bert_model.ckpt'\n",
    "dict_path = '/data/bert/vocab.txt'\n",
    "\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    D = []\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        f = f.read()\n",
    "        for l in f.split('\\n\\n'):\n",
    "            if not l:\n",
    "                continue\n",
    "            d, last_flag = [], ''\n",
    "            for c in l.split('\\n'):\n",
    "                try:\n",
    "                    char, this_flag = c.split(' ')\n",
    "                except:\n",
    "                    print(c)\n",
    "                    continue\n",
    "                if this_flag == 'O' and last_flag == 'O':\n",
    "                    d[-1][0] += char\n",
    "                elif this_flag == 'O' and last_flag != 'O':\n",
    "                    d.append([char, 'O'])\n",
    "                elif this_flag[:1] == 'B':\n",
    "                    d.append([char, this_flag[2:]])\n",
    "                else:\n",
    "                    d[-1][0] += char\n",
    "                last_flag = this_flag\n",
    "            D.append(d)\n",
    "    return D\n",
    "\n",
    "\n",
    "# 标注数据\n",
    "train_data = load_data('./round1_train/data/train.txt')\n",
    "valid_data = load_data('./round1_train/data/val.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立分词器\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer(dict_path, do_lower_case=True)\n",
    "\n",
    "# 类别映射\n",
    "\n",
    "labels = ['SYMPTOM',\n",
    " 'DRUG_EFFICACY',\n",
    " 'PERSON_GROUP',\n",
    " 'SYNDROME',\n",
    " 'DRUG_TASTE',\n",
    " 'DISEASE',\n",
    " 'DRUG_DOSAGE',\n",
    " 'DRUG_INGREDIENT',\n",
    " 'FOOD_GROUP',\n",
    " 'DISEASE_GROUP',\n",
    " 'DRUG',\n",
    " 'FOOD',\n",
    " 'DRUG_GROUP']\n",
    "\n",
    "id2label = dict(enumerate(labels))\n",
    "label2id = {j: i for i, j in id2label.items()}\n",
    "num_labels = len(labels) * 2 + 1\n",
    "\n",
    "\n",
    "class data_generator(DataGenerator):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "        for is_end, item in self.sample(random):\n",
    "            token_ids, labels = [tokenizer._token_start_id], [0]\n",
    "            for w, l in item:\n",
    "                w_token_ids = tokenizer.encode(w)[0][1:-1]\n",
    "                if len(token_ids) + len(w_token_ids) < maxlen:\n",
    "                    token_ids += w_token_ids\n",
    "                    if l == 'O':\n",
    "                        labels += [0] * len(w_token_ids)\n",
    "                    else:\n",
    "                        B = label2id[l] * 2 + 1\n",
    "                        I = label2id[l] * 2 + 2\n",
    "                        labels += ([B] + [I] * (len(w_token_ids) - 1))\n",
    "                else:\n",
    "                    break\n",
    "            token_ids += [tokenizer._token_end_id]\n",
    "            labels += [0]\n",
    "            segment_ids = [0] * len(token_ids)\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_segment_ids.append(segment_ids)\n",
    "            batch_labels.append(labels)\n",
    "            if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                batch_labels = sequence_padding(batch_labels)\n",
    "                yield [batch_token_ids, batch_segment_ids], batch_labels\n",
    "                batch_token_ids, batch_segment_ids, batch_labels = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  123 232 34434\n",
    "#  1, 2, 0\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 选用GPU序号\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qB0QPDNN5NbV",
    "outputId": "805bd22f-e7b3-4ddc-f895-6618e77dde67"
   },
   "outputs": [],
   "source": [
    "model = build_transformer_model(\n",
    "    config_path,\n",
    "    checkpoint_path,\n",
    ")\n",
    "\n",
    "output_layer = 'Transformer-%s-FeedForward-Norm' % (bert_layers - 1)\n",
    "output = model.get_layer(output_layer).output\n",
    "output = Dense(num_labels)(output) # 27分类\n",
    "\n",
    "CRF = ConditionalRandomField(lr_multiplier=crf_lr_multiplier)\n",
    "output = CRF(output)\n",
    "\n",
    "model = Model(model.input, output)\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss=CRF.sparse_loss,\n",
    "    optimizer=Adam(learing_rate),\n",
    "    metrics=[CRF.sparse_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7BgVz8TY5Nbj",
    "outputId": "1e37000a-967e-4c64-eadf-5d97e2e5c1b9"
   },
   "outputs": [],
   "source": [
    "class NamedEntityRecognizer(ViterbiDecoder):\n",
    "    \"\"\"命名实体识别器\n",
    "    \"\"\"\n",
    "    def recognize(self, text):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        mapping = tokenizer.rematch(text, tokens)\n",
    "        token_ids = tokenizer.tokens_to_ids(tokens)\n",
    "        segment_ids = [0] * len(token_ids)\n",
    "        token_ids, segment_ids = to_array([token_ids], [segment_ids])\n",
    "        nodes = model.predict([token_ids, segment_ids])[0]\n",
    "        labels = self.decode(nodes)\n",
    "        entities, starting = [], False\n",
    "        for i, label in enumerate(labels):\n",
    "            if label > 0:\n",
    "                if label % 2 == 1:\n",
    "                    starting = True\n",
    "                    entities.append([[i], id2label[(label - 1) // 2]])\n",
    "                elif starting:\n",
    "                    entities[-1][0].append(i)\n",
    "                else:\n",
    "                    starting = False\n",
    "            else:\n",
    "                starting = False\n",
    "\n",
    "        return [(text[mapping[w[0]][0]:mapping[w[-1]][-1] + 1], l)\n",
    "                for w, l in entities]\n",
    "\n",
    "\n",
    "NER = NamedEntityRecognizer(trans=K.eval(CRF.trans), starts=[0], ends=[0])\n",
    "\n",
    "\n",
    "def evaluate(data):\n",
    "    \"\"\"评测函数\n",
    "    \"\"\"\n",
    "    X, Y, Z = 1e-10, 1e-10, 1e-10\n",
    "    for d in tqdm(data):\n",
    "        text = ''.join([i[0] for i in d])\n",
    "        R = set(NER.recognize(text)) # 预测\n",
    "        T = set([tuple(i) for i in d if i[1] != 'O']) #真实\n",
    "        X += len(R & T) \n",
    "        Y += len(R) \n",
    "        Z += len(T)\n",
    "    precision, recall =  X / Y, X / Z\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    def __init__(self,valid_data):\n",
    "        self.best_val_f1 = 0\n",
    "        self.valid_data = valid_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        trans = K.eval(CRF.trans)\n",
    "        NER.trans = trans\n",
    "#         print(NER.trans)\n",
    "        f1, precision, recall = evaluate(self.valid_data)\n",
    "        # 保存最优\n",
    "        if f1 >= self.best_val_f1:\n",
    "            self.best_val_f1 = f1\n",
    "            model.save_weights('./weights/best_model_epoch_10_dynamic-lr.weights')\n",
    "        print(\n",
    "            'valid:  f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\\n' %\n",
    "            (f1, precision, recall, self.best_val_f1)\n",
    "        )\n",
    "\n",
    "# 动态调整学习率lr\n",
    "def scheduler(epoch):\n",
    "    lr = K.get_value(model.optimizer.lr)\n",
    "    # 学习率每轮减半\n",
    "    K.set_value(model.optimizer.lr, lr * 0.5)\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "lr_new = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "lr_schedular = keras.callbacks.ReduceLROnPlateau(\n",
    "                                monitor='val_loss',\n",
    "                                factor=0.1,\n",
    "                                patience=10,)\n",
    "\n",
    "evaluator = Evaluator(valid_data)\n",
    "train_generator = data_generator(train_data, batch_size)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator.forfit(),\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=epochs,\n",
    "    callbacks=[evaluator, lr_new]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cut(sentence):\n",
    "    \"\"\"\n",
    "    将一段文本切分成多个句子\n",
    "    :param sentence:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_sentence = []\n",
    "    sen = []\n",
    "    for i in sentence:\n",
    "        if i in ['。', '！', '？', '?'] and len(sen) != 0:\n",
    "            sen.append(i)\n",
    "            new_sentence.append(\"\".join(sen))\n",
    "            sen = []\n",
    "            continue\n",
    "        sen.append(i)\n",
    "\n",
    "    if len(new_sentence) <= 1: # 一句话超过max_seq_length且没有句号的，用\",\"分割，再长的不考虑了。\n",
    "        new_sentence = []\n",
    "        sen = []\n",
    "        for i in sentence:\n",
    "            if i.split(' ')[0] in ['，', ','] and len(sen) != 0:\n",
    "                sen.append(i)\n",
    "                new_sentence.append(\"\".join(sen))\n",
    "                sen = []\n",
    "                continue\n",
    "            sen.append(i)\n",
    "    if len(sen) > 0:  # 若最后一句话无结尾标点，则加入这句话\n",
    "        new_sentence.append(\"\".join(sen))\n",
    "    return new_sentence\n",
    "\n",
    "def cut_test_set(text_list,len_treshold):\n",
    "    cut_text_list = []\n",
    "    cut_index_list = []\n",
    "    for text in text_list:\n",
    "\n",
    "        temp_cut_text_list = []\n",
    "        text_agg = ''\n",
    "        if len(text) < len_treshold:\n",
    "            temp_cut_text_list.append(text)\n",
    "        else:\n",
    "            sentence_list = _cut(text)  # 一条数据被切分成多句话\n",
    "            for sentence in sentence_list:\n",
    "                if len(text_agg) + len(sentence) < len_treshold:\n",
    "                    text_agg += sentence\n",
    "                else:\n",
    "                    temp_cut_text_list.append(text_agg)\n",
    "                    text_agg = sentence\n",
    "            temp_cut_text_list.append(text_agg)  # 加上最后一个句子\n",
    "\n",
    "        cut_index_list.append(len(temp_cut_text_list))\n",
    "        cut_text_list += temp_cut_text_list\n",
    "\n",
    "    return cut_text_list, cut_index_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamedEntityRecognizer(ViterbiDecoder):\n",
    "    \"\"\"命名实体识别器\n",
    "    \"\"\"\n",
    "    def recognize(self, text):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        mapping = tokenizer.rematch(text, tokens)\n",
    "        token_ids = tokenizer.tokens_to_ids(tokens)\n",
    "        segment_ids = [0] * len(token_ids)\n",
    "        nodes = model.predict([[token_ids], [segment_ids]])[0]\n",
    "        labels = self.decode(nodes)\n",
    "        entities, starting = [], False\n",
    "        \n",
    "        for i, label in enumerate(labels):\n",
    "            if label > 0:\n",
    "                if label % 2 == 1:\n",
    "                    starting = True\n",
    "                    entities.append([[i], id2label[(label - 1) // 2]])\n",
    "                elif starting:\n",
    "                    entities[-1][0].append(i)\n",
    "                else:\n",
    "                    starting = False\n",
    "            else:\n",
    "                starting = False\n",
    "\n",
    "        return [(text[mapping[w[0]][0]:mapping[w[-1]][-1] + 1], l)\n",
    "                for w, l in entities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER = NamedEntityRecognizer(trans=K.eval(CRF.trans), starts=[0], ends=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict(data, NER_):\n",
    "    test_ner =[]\n",
    "    \n",
    "    for text in tqdm(data):\n",
    "        cut_text_list, cut_index_list = cut_test_set([text],maxlen)\n",
    "        posit = 0\n",
    "        item_ner = []\n",
    "        index =1\n",
    "        for str_ in cut_text_list:\n",
    "            aaaa  = NER_.recognize(str_)\n",
    "            for tn in aaaa:\n",
    "                ans = {}\n",
    "                ans[\"label_type\"] = tn[1]\n",
    "                ans['overlap'] = \"T\" + str(index)\n",
    "                \n",
    "                ans[\"start_pos\"] = text.find(tn[0],posit)\n",
    "                ans[\"end_pos\"] = ans[\"start_pos\"] + len(tn[0])\n",
    "                posit = ans[\"end_pos\"]\n",
    "                ans[\"res\"] = tn[0]\n",
    "                item_ner.append(ans)\n",
    "                index +=1\n",
    "        test_ner.append(item_ner)\n",
    "    \n",
    "    return test_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import codecs\n",
    "X, Y, Z = 1e-10, 1e-10, 1e-10\n",
    "val_data_flist = glob.glob('./round1_train/val_data/*.txt')\n",
    "data_dir = './round1_train/val_data/'\n",
    "for file in val_data_flist:\n",
    "    if file.find(\".ann\") == -1 and file.find(\".txt\") == -1:\n",
    "        continue\n",
    "    file_name = file.split('/')[-1].split('.')[0]\n",
    "    r_ann_path = os.path.join(data_dir, \"%s.ann\" % file_name)\n",
    "    r_txt_path = os.path.join(data_dir, \"%s.txt\" % file_name)\n",
    "\n",
    "    R = []\n",
    "    with codecs.open(r_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        line = f.readlines()\n",
    "        aa = test_predict(line, NER)\n",
    "        for line in aa[0]:\n",
    "            lines = line['label_type']+ \" \"+str(line['start_pos'])+' ' +str(line['end_pos'])+ \"\\t\" +line['res']\n",
    "            R.append(lines)    \n",
    "    T = []\n",
    "    with codecs.open(r_ann_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            lines = line.strip('\\n').split('\\t')[1] + '\\t' + line.strip('\\n').split('\\t')[2]\n",
    "            T.append(lines)\n",
    "    R = set(R)\n",
    "    T = set(T)\n",
    "    X += len(R & T) \n",
    "    Y += len(R) \n",
    "    Z += len(T)\n",
    "precision, recall =  X / Y, X / Z\n",
    "f1 = 2*precision*recall/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1,precision,recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NraWZf1vLsGP"
   },
   "outputs": [],
   "source": [
    "class NamedEntityRecognizer(ViterbiDecoder):\n",
    "    \"\"\"命名实体识别器\n",
    "    \"\"\"\n",
    "    def recognize(self, text):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        mapping = tokenizer.rematch(text, tokens)\n",
    "        token_ids = tokenizer.tokens_to_ids(tokens)\n",
    "        segment_ids = [0] * len(token_ids)\n",
    "        nodes = model.predict([[token_ids], [segment_ids]])[0]\n",
    "        labels = self.decode(nodes)\n",
    "        entities, starting = [], False\n",
    "        \n",
    "        for i, label in enumerate(labels):\n",
    "            if label > 0:\n",
    "                if label % 2 == 1:\n",
    "                    starting = True\n",
    "                    entities.append([[i], id2label[(label - 1) // 2]])\n",
    "                elif starting:\n",
    "                    entities[-1][0].append(i)\n",
    "                else:\n",
    "                    starting = False\n",
    "            else:\n",
    "                starting = False\n",
    "\n",
    "        return [(text[mapping[w[0]][0]:mapping[w[-1]][-1] + 1], l)\n",
    "                for w, l in entities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "831riK43Lmfk"
   },
   "outputs": [],
   "source": [
    "NER = NamedEntityRecognizer(trans=K.eval(CRF.trans), starts=[0], ends=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict(data, NER_):\n",
    "    test_ner =[]\n",
    "    \n",
    "    for text in tqdm(data):\n",
    "        cut_text_list, cut_index_list = cut_test_set([text],maxlen)\n",
    "        posit = 0\n",
    "        item_ner = []\n",
    "        index =1\n",
    "        for str_ in cut_text_list:\n",
    "            ner_res  = NER_.recognize(str_)\n",
    "            for tn in ner_res:\n",
    "                ans = {}\n",
    "                ans[\"label_type\"] = tn[1]\n",
    "                ans['overlap'] = \"T\" + str(index)\n",
    "                \n",
    "                ans[\"start_pos\"] = text.find(tn[0],posit)\n",
    "                ans[\"end_pos\"] = ans[\"start_pos\"] + len(tn[0])\n",
    "                posit = ans[\"end_pos\"]\n",
    "                ans[\"res\"] = tn[0]\n",
    "                item_ner.append(ans)\n",
    "                index +=1\n",
    "        test_ner.append(item_ner)\n",
    "    \n",
    "    return test_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hBI0ja-hY7M6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJROaaisY9p2"
   },
   "outputs": [],
   "source": [
    "test_files = os.listdir(\"./round1_test/chusai_xuanshou/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Q8OGwD9GZRES",
    "outputId": "7d41645c-2d56-4456-bb04-0496d1d5609f"
   },
   "outputs": [],
   "source": [
    "for file in test_files:\n",
    "    with codecs.open(\"./round1_test/chusai_xuanshou/\"+file, \"r\", encoding=\"utf-8\") as f:\n",
    "        line = f.readlines()\n",
    "        aa = test_predict(line, NER)\n",
    "    with codecs.open(\"./round1_test/submission_4/\"+file.split('.')[0]+\".ann\", \"w\", encoding=\"utf-8\") as ff:\n",
    "        for line in aa[0]:\n",
    "            lines = line['overlap'] + \"\\t\" +line['label_type']+ \" \"+str(line['start_pos'])+' ' +str(line['end_pos'])+ \"\\t\" +line['res']\n",
    "            ff.write(lines+\"\\n\")\n",
    "        ff.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
