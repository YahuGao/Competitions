{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sample = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "START = '<START>'\n",
    "END = '<END>'\n",
    "UNKNOWN = '<UNKNOWN>'\n",
    "PAD = '<PAD>'\n",
    "\n",
    "texts = pd.concat([train.text, test.text], axis=0)\n",
    "word2idx = {'<START>': 0, '<END>': 1, '<UNKNOWN': 2, '<PAD>': 3}  # word and its count\n",
    "sentence_max_length = 0\n",
    "\n",
    "for text in texts:\n",
    "    sentence_length = len(text.strip().split())\n",
    "    if sentence_length > sentence_max_length:\n",
    "        sentence_max_length = sentence_length\n",
    "    for word in text.strip().split():\n",
    "        if not word2idx.get(word, None):\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "vocab_length = len(word2idx)\n",
    "# Add <start> and <end> token\n",
    "sentence_max_length += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    inputs, y = zip(*data)\n",
    "    inputs_len = [len(item) for item in inputs]\n",
    "    inputs = rnn_utils.pad_sequence(inputs, batch_first=True, padding_value=torch.tensor(3))\n",
    "    labels = torch.LongTensor(y[:len(inputs_len)])\n",
    "    return inputs, inputs_len, labels\n",
    "\n",
    "class myDataSet():\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = list(inputs)\n",
    "        self.labels = list(labels)\n",
    "\n",
    "    def to_categorical(y, num_classes):\n",
    "        return np.eye(num_classes=lasses, dtype='uint8')[y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.inputs[index]\n",
    "        inputs = [word2idx.get(word, word2idx.get('<UNKNOWN>')) for word in sentence.split()]\n",
    "\n",
    "        inputs.insert(0, word2idx.get('<START>'))\n",
    "        inputs.append(word2idx.get('<END>'))\n",
    "        while sentence_max_length - len(inputs) > 0:\n",
    "            inputs.insert(0, word2idx.get('<PAD>'))\n",
    "        label = self.labels[index]\n",
    "        return torch.tensor(inputs), torch.tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.loc[:, ['text', 'target']].sample(frac=1)\n",
    "train_dataset =  myDataSet(train.text, train.target)\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_data.text, train_data.target,\n",
    "                                                 test_size=0.3, random_state=0)\n",
    "\n",
    "train_dataset = myDataSet(train_x, train_y)\n",
    "val_dataset = myDataSet(val_x, val_y)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True,\n",
    "                                           batch_size=BATCH_SIZE) #, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, shuffle=True,\n",
    "                                         batch_size=BATCH_SIZE) #, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=300,\n",
    "                hidden_size=128,\n",
    "                num_layers=2,\n",
    "                drop_p=0.5,\n",
    "                batch_first=True,\n",
    "                bidirectional=False,\n",
    "                output_size=3):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(len(word2idx), input_size)\n",
    "\n",
    "        self.LSTM = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           batch_first=batch_first,\n",
    "                           dropout=drop_p)\n",
    "        self.direction = 1\n",
    "        if bidirectional:\n",
    "            self.direction = 2\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size * self.direction, 1)\n",
    "        self.fc2 = nn.Linear(sentence_max_length, 1)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out, _ = self.LSTM(out)\n",
    "#         out_pad, out_pad_len = rnn_utils.pad_packed_sequence(out, batch_first=True)\n",
    "#         out = out_pad[:, out_pad_len-1, :]\n",
    "        out = self.fc1(out)\n",
    "        # out = out.view(out.size()[0], -1)\n",
    "        out = out[:, -1]\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "drop_p = 0.5\n",
    "output_size = 1\n",
    "bidirectional = True\n",
    "batch_first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LSTM(input_size,\n",
    "          hidden_size,\n",
    "          num_layers,\n",
    "          drop_p,\n",
    "          batch_first,\n",
    "          bidirectional,\n",
    "          output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "count = 0\n",
    "lr = 0.005\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for item in range(1):\n",
    "    train_losses = []\n",
    "    lr = lr - 0.9 * (item % 2) * lr\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "    for inputs, y in train_loader:\n",
    "        # inputs = rnn_utils.pack_padded_sequence(inputs, inputs_lengths, batch_first=True)\n",
    "        inputs = inputs.to(device)\n",
    "        y = y.to(device)\n",
    "        count += 1\n",
    "        net.zero_grad()\n",
    "        out = net(inputs)\n",
    "        print(\"Out size:\", out.shape)\n",
    "        print(\"out\", out)\n",
    "        break\n",
    "        loss = criterion(out, y)\n",
    "        train_losses.append(loss.cpu().item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if count % 10 == 0:\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for val_x, val_y in val_loader:\n",
    "                # val_x = rnn_utils.pack_padded_sequence(val_x, val_x_lengths, batch_first=True)\n",
    "                val_x = val_x.to(device)\n",
    "                val_y = val_y.to(device)\n",
    "                val_out = net(val_x)\n",
    "                val_loss = criterion(val_out, val_y)\n",
    "                val_losses.append(val_loss.cpu().item())\n",
    "                # val_acc = accuracy_score(val_y.data.cpu().numpy(), val_out.cpu().numpy() > 0.5)\n",
    "            net.train()\n",
    "            print(\"EPOCH: {:d} Train_loss: {:.6f} Val loss: {:.6f}\".format(\n",
    "                item, np.mean(train_losses), np.mean(val_losses)))\n",
    "            \n",
    "            if np.mean(val_losses) < valid_loss_min:\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "                print(\"Validation loss decreased({: .6f} --> {: .6f}) \\\n",
    "                      saving model ...\".format(valid_loss_min, np.mean(val_losses)))\n",
    "                torch.save(net.state_dict(), 'model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
