{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sample = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "START = '<START>'\n",
    "END = '<END>'\n",
    "UNKNOWN = '<UNKNOWN>'\n",
    "PAD = '<PAD>'\n",
    "\n",
    "texts = pd.concat([train.text, test.text], axis=0)\n",
    "word2idx = {'<START>': 0, '<END>': 1, '<UNKNOWN': 2, '<PAD>': 3}  # word and its count\n",
    "sentence_max_length = 0\n",
    "\n",
    "for text in texts:\n",
    "    sentence_length = len(text.strip().split())\n",
    "    if sentence_length > sentence_max_length:\n",
    "        sentence_max_length = sentence_length\n",
    "    for word in text.strip().split():\n",
    "        if not word2idx.get(word, None):\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "vocab_length = len(word2idx)\n",
    "# Add <start> and <end> token\n",
    "sentence_max_length += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    inputs, y = zip(*data)\n",
    "    inputs_len = [len(item) for item in inputs]\n",
    "    inputs = rnn_utils.pad_sequence(inputs, batch_first=True, padding_value=torch.tensor(3))\n",
    "    labels = torch.LongTensor(y[:len(inputs_len)])\n",
    "    return inputs, inputs_len, labels\n",
    "\n",
    "class myDataSet():\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = list(inputs)\n",
    "        self.labels = list(labels)\n",
    "\n",
    "    def to_categorical(y, num_classes):\n",
    "        return np.eye(num_classes=lasses, dtype='uint8')[y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.inputs[index]\n",
    "\n",
    "        label = self.labels[index]\n",
    "        return torch.tensor(inputs), torch.tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.loc[:, ['text', 'target']].sample(frac=1)\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_data.text, train_data.target,\n",
    "                                                 test_size=0.3, random_state=0)\n",
    "\n",
    "train_dataset = myDataSet(train_x, train_y)\n",
    "val_dataset = myDataSet(val_x, val_y)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True,\n",
    "#                                            batch_size=BATCH_SIZE) #, collate_fn=collate_fn)\n",
    "# val_loader = torch.utils.data.DataLoader(val_dataset, shuffle=True,\n",
    "#                                          batch_size=BATCH_SIZE) #, collate_fn=collate_fn)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=300,\n",
    "                hidden_size=128,\n",
    "                num_layers=2,\n",
    "                drop_p=0.5,\n",
    "                batch_first=True,\n",
    "                bidirectional=False,\n",
    "                output_size=3):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(len(word2idx), input_size)\n",
    "\n",
    "        self.LSTM = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           batch_first=batch_first,\n",
    "                           dropout=drop_p)\n",
    "        self.direction = 1\n",
    "        if bidirectional:\n",
    "            self.direction = 2\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size * self.direction, 1)\n",
    "        self.fc2 = nn.Linear(sentence_max_length, 1)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out, _ = self.LSTM(out)\n",
    "#         out_pad, out_pad_len = rnn_utils.pad_packed_sequence(out, batch_first=True)\n",
    "#         out = out_pad[:, out_pad_len-1, :]\n",
    "        # print(out.size())\n",
    "        out = self.fc1(out)\n",
    "        # print(out.size())\n",
    "        out = out.view(out.size()[0], out.size()[1])\n",
    "        # print(out.size())\n",
    "        out = self.fc2(out)\n",
    "        # print(out.size())\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "drop_p = 0.5\n",
    "output_size = 1\n",
    "bidirectional = True\n",
    "batch_first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LSTM(input_size,\n",
    "          hidden_size,\n",
    "          num_layers,\n",
    "          drop_p,\n",
    "          batch_first,\n",
    "          bidirectional,\n",
    "          output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 Train_loss: 0.684184 Val loss: 0.706159\n",
      "Validation loss decreased( inf -->  0.706159)                       saving model ...\n",
      "EPOCH: 0 Train_loss: 0.691368 Val loss: 0.684579\n",
      "Validation loss decreased( 0.706159 -->  0.684579)                       saving model ...\n",
      "EPOCH: 0 Train_loss: 0.689315 Val loss: 0.679105\n",
      "Validation loss decreased( 0.684579 -->  0.679105)                       saving model ...\n",
      "EPOCH: 0 Train_loss: 0.684610 Val loss: 0.687360\n",
      "EPOCH: 0 Train_loss: 0.683112 Val loss: 0.675501\n",
      "Validation loss decreased( 0.679105 -->  0.675501)                       saving model ...\n",
      "EPOCH: 0 Train_loss: 0.681550 Val loss: 0.647548\n",
      "Validation loss decreased( 0.675501 -->  0.647548)                       saving model ...\n",
      "EPOCH: 0 Train_loss: 0.672905 Val loss: 0.634271\n",
      "Validation loss decreased( 0.647548 -->  0.634271)                       saving model ...\n",
      "EPOCH: 0 Train_loss: 0.670046 Val loss: 0.631773\n",
      "Validation loss decreased( 0.634271 -->  0.631773)                       saving model ...\n",
      "EPOCH: 0 Train_loss: 0.665340 Val loss: 0.600754\n",
      "Validation loss decreased( 0.631773 -->  0.600754)                       saving model ...\n",
      "EPOCH: 0 Train_loss: 0.657492 Val loss: 0.585118\n",
      "Validation loss decreased( 0.600754 -->  0.585118)                       saving model ...\n",
      "EPOCH: 0 Train_loss: 0.652512 Val loss: 0.598634\n",
      "EPOCH: 0 Train_loss: 0.647841 Val loss: 0.581158\n",
      "Validation loss decreased( 0.585118 -->  0.581158)                       saving model ...\n",
      "EPOCH: 0 Train_loss: 0.641175 Val loss: 0.568201\n",
      "Validation loss decreased( 0.581158 -->  0.568201)                       saving model ...\n",
      "EPOCH: 0 Train_loss: 0.636545 Val loss: 0.566353\n",
      "Validation loss decreased( 0.568201 -->  0.566353)                       saving model ...\n",
      "EPOCH: 0 Train_loss: 0.631528 Val loss: 0.597716\n",
      "EPOCH: 0 Train_loss: 0.629973 Val loss: 0.582050\n",
      "EPOCH: 1 Train_loss: 0.512658 Val loss: 0.568940\n",
      "EPOCH: 1 Train_loss: 0.465688 Val loss: 0.548972\n",
      "Validation loss decreased( 0.566353 -->  0.548972)                       saving model ...\n",
      "EPOCH: 1 Train_loss: 0.474379 Val loss: 0.569321\n",
      "EPOCH: 1 Train_loss: 0.494996 Val loss: 0.564734\n",
      "EPOCH: 1 Train_loss: 0.504260 Val loss: 0.556518\n",
      "EPOCH: 1 Train_loss: 0.498226 Val loss: 0.558066\n",
      "EPOCH: 1 Train_loss: 0.491768 Val loss: 0.549988\n",
      "EPOCH: 1 Train_loss: 0.486909 Val loss: 0.551453\n",
      "EPOCH: 1 Train_loss: 0.491872 Val loss: 0.554858\n",
      "EPOCH: 1 Train_loss: 0.494000 Val loss: 0.531196\n",
      "Validation loss decreased( 0.548972 -->  0.531196)                       saving model ...\n",
      "EPOCH: 1 Train_loss: 0.491273 Val loss: 0.523842\n",
      "Validation loss decreased( 0.531196 -->  0.523842)                       saving model ...\n",
      "EPOCH: 1 Train_loss: 0.491880 Val loss: 0.516301\n",
      "Validation loss decreased( 0.523842 -->  0.516301)                       saving model ...\n",
      "EPOCH: 1 Train_loss: 0.488508 Val loss: 0.524512\n",
      "EPOCH: 1 Train_loss: 0.492556 Val loss: 0.523270\n",
      "EPOCH: 1 Train_loss: 0.491109 Val loss: 0.526047\n",
      "EPOCH: 1 Train_loss: 0.490396 Val loss: 0.526135\n",
      "EPOCH: 1 Train_loss: 0.493091 Val loss: 0.507602\n",
      "Validation loss decreased( 0.516301 -->  0.507602)                       saving model ...\n",
      "EPOCH: 2 Train_loss: 0.322338 Val loss: 0.613852\n",
      "EPOCH: 2 Train_loss: 0.337331 Val loss: 0.551888\n",
      "EPOCH: 2 Train_loss: 0.357913 Val loss: 0.561076\n",
      "EPOCH: 2 Train_loss: 0.381047 Val loss: 0.513443\n",
      "EPOCH: 2 Train_loss: 0.391583 Val loss: 0.525797\n",
      "EPOCH: 2 Train_loss: 0.394996 Val loss: 0.581890\n",
      "EPOCH: 2 Train_loss: 0.409679 Val loss: 0.509421\n",
      "EPOCH: 2 Train_loss: 0.419743 Val loss: 0.505513\n",
      "Validation loss decreased( 0.507602 -->  0.505513)                       saving model ...\n",
      "EPOCH: 2 Train_loss: 0.425919 Val loss: 0.517873\n",
      "EPOCH: 2 Train_loss: 0.426964 Val loss: 0.526911\n",
      "EPOCH: 2 Train_loss: 0.432535 Val loss: 0.504507\n",
      "Validation loss decreased( 0.505513 -->  0.504507)                       saving model ...\n",
      "EPOCH: 2 Train_loss: 0.435878 Val loss: 0.491655\n",
      "Validation loss decreased( 0.504507 -->  0.491655)                       saving model ...\n",
      "EPOCH: 2 Train_loss: 0.436976 Val loss: 0.522133\n",
      "EPOCH: 2 Train_loss: 0.438772 Val loss: 0.484227\n",
      "Validation loss decreased( 0.491655 -->  0.484227)                       saving model ...\n",
      "EPOCH: 2 Train_loss: 0.439045 Val loss: 0.480265\n",
      "Validation loss decreased( 0.484227 -->  0.480265)                       saving model ...\n",
      "EPOCH: 2 Train_loss: 0.444285 Val loss: 0.493624\n",
      "EPOCH: 2 Train_loss: 0.445899 Val loss: 0.476857\n",
      "Validation loss decreased( 0.480265 -->  0.476857)                       saving model ...\n",
      "EPOCH: 3 Train_loss: 0.312757 Val loss: 0.527408\n",
      "EPOCH: 3 Train_loss: 0.360009 Val loss: 0.544553\n",
      "EPOCH: 3 Train_loss: 0.395102 Val loss: 0.532727\n",
      "EPOCH: 3 Train_loss: 0.410298 Val loss: 0.542070\n",
      "EPOCH: 3 Train_loss: 0.414485 Val loss: 0.525796\n",
      "EPOCH: 3 Train_loss: 0.422890 Val loss: 0.505308\n",
      "EPOCH: 3 Train_loss: 0.426175 Val loss: 0.506597\n",
      "EPOCH: 3 Train_loss: 0.429157 Val loss: 0.500716\n",
      "EPOCH: 3 Train_loss: 0.427397 Val loss: 0.523982\n",
      "EPOCH: 3 Train_loss: 0.426566 Val loss: 0.498984\n",
      "EPOCH: 3 Train_loss: 0.432851 Val loss: 0.507261\n",
      "EPOCH: 3 Train_loss: 0.436474 Val loss: 0.488841\n",
      "EPOCH: 3 Train_loss: 0.438352 Val loss: 0.478394\n",
      "EPOCH: 3 Train_loss: 0.436114 Val loss: 0.500246\n",
      "EPOCH: 3 Train_loss: 0.434044 Val loss: 0.479660\n",
      "EPOCH: 3 Train_loss: 0.432341 Val loss: 0.487795\n",
      "EPOCH: 4 Train_loss: 0.239848 Val loss: 0.533570\n",
      "EPOCH: 4 Train_loss: 0.343613 Val loss: 0.537159\n",
      "EPOCH: 4 Train_loss: 0.413999 Val loss: 0.563275\n",
      "EPOCH: 4 Train_loss: 0.428214 Val loss: 0.526009\n",
      "EPOCH: 4 Train_loss: 0.429489 Val loss: 0.588917\n",
      "EPOCH: 4 Train_loss: 0.433249 Val loss: 0.515317\n",
      "EPOCH: 4 Train_loss: 0.433475 Val loss: 0.510929\n",
      "EPOCH: 4 Train_loss: 0.439234 Val loss: 0.507647\n",
      "EPOCH: 4 Train_loss: 0.444055 Val loss: 0.500498\n",
      "EPOCH: 4 Train_loss: 0.442985 Val loss: 0.503398\n",
      "EPOCH: 4 Train_loss: 0.444117 Val loss: 0.500616\n",
      "EPOCH: 4 Train_loss: 0.446101 Val loss: 0.484477\n",
      "EPOCH: 4 Train_loss: 0.440310 Val loss: 0.500346\n",
      "EPOCH: 4 Train_loss: 0.447524 Val loss: 0.501482\n",
      "EPOCH: 4 Train_loss: 0.448456 Val loss: 0.486326\n",
      "EPOCH: 4 Train_loss: 0.450156 Val loss: 0.481798\n",
      "EPOCH: 4 Train_loss: 0.451441 Val loss: 0.475634\n",
      "Validation loss decreased( 0.476857 -->  0.475634)                       saving model ...\n",
      "EPOCH: 5 Train_loss: 0.251732 Val loss: 0.522987\n",
      "EPOCH: 5 Train_loss: 0.339763 Val loss: 0.535008\n",
      "EPOCH: 5 Train_loss: 0.397744 Val loss: 0.516106\n",
      "EPOCH: 5 Train_loss: 0.413051 Val loss: 0.524964\n",
      "EPOCH: 5 Train_loss: 0.430745 Val loss: 0.512967\n",
      "EPOCH: 5 Train_loss: 0.439285 Val loss: 0.497770\n",
      "EPOCH: 5 Train_loss: 0.441078 Val loss: 0.487899\n",
      "EPOCH: 5 Train_loss: 0.439695 Val loss: 0.491758\n",
      "EPOCH: 5 Train_loss: 0.441061 Val loss: 0.480821\n",
      "EPOCH: 5 Train_loss: 0.439758 Val loss: 0.475980\n",
      "EPOCH: 5 Train_loss: 0.448119 Val loss: 0.465207\n",
      "Validation loss decreased( 0.475634 -->  0.465207)                       saving model ...\n",
      "EPOCH: 5 Train_loss: 0.447856 Val loss: 0.481175\n",
      "EPOCH: 5 Train_loss: 0.448148 Val loss: 0.470197\n",
      "EPOCH: 5 Train_loss: 0.447915 Val loss: 0.474502\n",
      "EPOCH: 5 Train_loss: 0.448375 Val loss: 0.474304\n",
      "EPOCH: 5 Train_loss: 0.446860 Val loss: 0.484095\n",
      "EPOCH: 5 Train_loss: 0.448366 Val loss: 0.466892\n",
      "EPOCH: 6 Train_loss: 0.263701 Val loss: 0.586656\n",
      "EPOCH: 6 Train_loss: 0.360106 Val loss: 0.531679\n",
      "EPOCH: 6 Train_loss: 0.403645 Val loss: 0.537006\n",
      "EPOCH: 6 Train_loss: 0.418463 Val loss: 0.494918\n",
      "EPOCH: 6 Train_loss: 0.425042 Val loss: 0.501964\n",
      "EPOCH: 6 Train_loss: 0.426605 Val loss: 0.496076\n",
      "EPOCH: 6 Train_loss: 0.428759 Val loss: 0.501726\n",
      "EPOCH: 6 Train_loss: 0.435321 Val loss: 0.493977\n",
      "EPOCH: 6 Train_loss: 0.435050 Val loss: 0.481185\n",
      "EPOCH: 6 Train_loss: 0.436482 Val loss: 0.479563\n",
      "EPOCH: 6 Train_loss: 0.437310 Val loss: 0.487401\n",
      "EPOCH: 6 Train_loss: 0.437095 Val loss: 0.466446\n",
      "EPOCH: 6 Train_loss: 0.438520 Val loss: 0.501759\n",
      "EPOCH: 6 Train_loss: 0.439695 Val loss: 0.478595\n",
      "EPOCH: 6 Train_loss: 0.441824 Val loss: 0.474197\n",
      "EPOCH: 6 Train_loss: 0.441002 Val loss: 0.472944\n",
      "EPOCH: 7 Train_loss: 0.216388 Val loss: 0.472440\n",
      "EPOCH: 7 Train_loss: 0.239552 Val loss: 0.566565\n",
      "EPOCH: 7 Train_loss: 0.363278 Val loss: 0.521035\n",
      "EPOCH: 7 Train_loss: 0.405559 Val loss: 0.503194\n",
      "EPOCH: 7 Train_loss: 0.419471 Val loss: 0.502893\n",
      "EPOCH: 7 Train_loss: 0.425373 Val loss: 0.526530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 7 Train_loss: 0.428150 Val loss: 0.497454\n",
      "EPOCH: 7 Train_loss: 0.429851 Val loss: 0.513973\n",
      "EPOCH: 7 Train_loss: 0.428085 Val loss: 0.516246\n",
      "EPOCH: 7 Train_loss: 0.432829 Val loss: 0.472798\n",
      "EPOCH: 7 Train_loss: 0.435963 Val loss: 0.472623\n",
      "EPOCH: 7 Train_loss: 0.432334 Val loss: 0.492175\n",
      "EPOCH: 7 Train_loss: 0.434987 Val loss: 0.474223\n",
      "EPOCH: 7 Train_loss: 0.433697 Val loss: 0.473744\n",
      "EPOCH: 7 Train_loss: 0.438027 Val loss: 0.462369\n",
      "Validation loss decreased( 0.465207 -->  0.462369)                       saving model ...\n",
      "EPOCH: 7 Train_loss: 0.439256 Val loss: 0.463805\n",
      "EPOCH: 7 Train_loss: 0.439253 Val loss: 0.498222\n",
      "EPOCH: 8 Train_loss: 0.251571 Val loss: 0.596678\n",
      "EPOCH: 8 Train_loss: 0.349007 Val loss: 0.504374\n",
      "EPOCH: 8 Train_loss: 0.403694 Val loss: 0.535137\n",
      "EPOCH: 8 Train_loss: 0.420785 Val loss: 0.509200\n",
      "EPOCH: 8 Train_loss: 0.428089 Val loss: 0.524233\n",
      "EPOCH: 8 Train_loss: 0.429655 Val loss: 0.496431\n",
      "EPOCH: 8 Train_loss: 0.434536 Val loss: 0.506635\n",
      "EPOCH: 8 Train_loss: 0.429347 Val loss: 0.504483\n",
      "EPOCH: 8 Train_loss: 0.431551 Val loss: 0.485264\n",
      "EPOCH: 8 Train_loss: 0.438425 Val loss: 0.507966\n",
      "EPOCH: 8 Train_loss: 0.438961 Val loss: 0.485844\n",
      "EPOCH: 8 Train_loss: 0.434269 Val loss: 0.489508\n",
      "EPOCH: 8 Train_loss: 0.424659 Val loss: 0.580441\n",
      "EPOCH: 8 Train_loss: 0.425980 Val loss: 0.470486\n",
      "EPOCH: 8 Train_loss: 0.427950 Val loss: 0.469606\n",
      "EPOCH: 8 Train_loss: 0.427006 Val loss: 0.467538\n",
      "EPOCH: 8 Train_loss: 0.427985 Val loss: 0.485970\n",
      "EPOCH: 9 Train_loss: 0.211390 Val loss: 0.690038\n",
      "EPOCH: 9 Train_loss: 0.283506 Val loss: 0.608382\n",
      "EPOCH: 9 Train_loss: 0.371981 Val loss: 0.572318\n",
      "EPOCH: 9 Train_loss: 0.390321 Val loss: 0.547012\n",
      "EPOCH: 9 Train_loss: 0.395524 Val loss: 0.535208\n",
      "EPOCH: 9 Train_loss: 0.405346 Val loss: 0.508031\n",
      "EPOCH: 9 Train_loss: 0.407082 Val loss: 0.524216\n",
      "EPOCH: 9 Train_loss: 0.415639 Val loss: 0.494804\n",
      "EPOCH: 9 Train_loss: 0.423089 Val loss: 0.484553\n",
      "EPOCH: 9 Train_loss: 0.425499 Val loss: 0.488520\n",
      "EPOCH: 9 Train_loss: 0.425221 Val loss: 0.494572\n",
      "EPOCH: 9 Train_loss: 0.426218 Val loss: 0.476053\n",
      "EPOCH: 9 Train_loss: 0.425361 Val loss: 0.480167\n",
      "EPOCH: 9 Train_loss: 0.425508 Val loss: 0.476558\n",
      "EPOCH: 9 Train_loss: 0.427865 Val loss: 0.467998\n",
      "EPOCH: 9 Train_loss: 0.427170 Val loss: 0.467221\n",
      "EPOCH: 9 Train_loss: 0.427398 Val loss: 0.480646\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "count = 0\n",
    "lr = 0.005\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for item in range(10):\n",
    "    train_losses = []\n",
    "    # lr = lr - 0.9 * (item % 2) * lr\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = lr, weight_decay=0.0001)\n",
    "    for inputs, y in train_loader:\n",
    "        # inputs = rnn_utils.pack_padded_sequence(inputs, inputs_lengths, batch_first=True)\n",
    "        inputs = inputs.to(device)\n",
    "        y = y.to(device)\n",
    "        count += 1\n",
    "        net.zero_grad()\n",
    "        out = net(inputs)\n",
    "\n",
    "        loss = criterion(out.flatten(), y)\n",
    "        train_losses.append(loss.cpu().item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if count % 10 == 0:\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for val_x, val_y in val_loader:\n",
    "                # val_x = rnn_utils.pack_padded_sequence(val_x, val_x_lengths, batch_first=True)\n",
    "                val_x = val_x.to(device)\n",
    "                val_y = val_y.to(device)\n",
    "                val_out = net(val_x)\n",
    "                val_loss = criterion(val_out.flatten(), val_y)\n",
    "                val_losses.append(val_loss.cpu().item())\n",
    "                # val_acc = accuracy_score(val_y.data.cpu().numpy(), val_out.cpu().numpy() > 0.5)\n",
    "            net.train()\n",
    "#             print(\"EPOCH: {:d} Train_loss: {:.6f} Val loss: {:.6f}\".format(\n",
    "#                 item, np.mean(train_losses), np.mean(val_losses)))\n",
    "            \n",
    "            if np.mean(val_losses) < valid_loss_min:\n",
    "                print(\"Validation loss decreased({: .6f} --> {: .6f}) \\\n",
    "                      saving model ...\".format(valid_loss_min, np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "                torch.save(net.state_dict(), 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263\n"
     ]
    }
   ],
   "source": [
    "test_texts = test.text\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(test)):\n",
    "    sentence = test.text[idx]\n",
    "    inputs = [word2idx.get(word, word2idx.get('<UNKNOWN>')) for word in sentence.split()]\n",
    "\n",
    "    inputs.insert(0, word2idx.get('<START>'))\n",
    "    inputs.append(word2idx.get('<END>'))\n",
    "    while sentence_max_length - len(inputs) > 0:\n",
    "        inputs.append(word2idx.get('<PAD>'))\n",
    "    \n",
    "    out = net(torch.tensor([inputs]))\n",
    "    if out.data > 0.5:\n",
    "        test.loc[idx, 'target'] = 1\n",
    "    else:\n",
    "        test.loc[idx, 'target'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['target'] = test.loc[:, 'target'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1\n",
      "1       0\n",
      "2       1\n",
      "3       0\n",
      "4       1\n",
      "       ..\n",
      "3258    0\n",
      "3259    1\n",
      "3260    1\n",
      "3261    1\n",
      "3262    0\n",
      "Name: target, Length: 3263, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test.loc[:, 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id', 'target']].to_csv('results.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
